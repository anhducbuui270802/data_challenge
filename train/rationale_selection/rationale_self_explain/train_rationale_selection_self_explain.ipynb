{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NZtF1pnc723u","outputId":"ed653d09-ff43-4525-8d8b-2c334e5a9d8d","executionInfo":{"status":"ok","timestamp":1698943762768,"user_tz":-420,"elapsed":3382,"user":{"displayName":"Anh Đức Bùi","userId":"02440975349884324274"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","[Errno 2] No such file or directory: '/content/drive/MyDrive/FEVER'\n","/content\n"]}],"source":["from google.colab import drive\n","\n","drive.mount('/content/drive')\n","%cd /content/drive/MyDrive/FEVER"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UWcCbpdb7UpD","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1698943815424,"user_tz":-420,"elapsed":52659,"user":{"displayName":"Anh Đức Bùi","userId":"02440975349884324274"}},"outputId":"a8f9af77-9f43-4c98-8687-94f2010cd690"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.1)\n","Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n","Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.3.2)\n","Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.6.3)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.1)\n","Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.1.0+cu118)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.12.4)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.5.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n","Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.1.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.3)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (1.11.3)\n","Requirement already satisfied: numpy<1.28.0,>=1.21.6 in /usr/local/lib/python3.10/dist-packages (from scipy) (1.23.5)\n","Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.34.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.4)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.17.3)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n","Requirement already satisfied: tokenizers<0.15,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.14.1)\n","Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.0)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n","Collecting sklearn\n","  Using cached sklearn-0.0.post10.tar.gz (3.6 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Building wheels for collected packages: sklearn\n","  Building wheel for sklearn (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for sklearn: filename=sklearn-0.0.post10-py3-none-any.whl size=2959 sha256=152ffc97a1c773bd4efa2c4a0493c1c6bc0e15514da93fd7044ab11c8335eaea\n","  Stored in directory: /root/.cache/pip/wheels/5b/f6/92/0173054cc528db7ffe7b0c7652a96c3102aab156a6da960387\n","Successfully built sklearn\n","Installing collected packages: sklearn\n","Successfully installed sklearn-0.0.post10\n"]}],"source":["!pip install tqdm\n","!pip install nltk\n","!pip install torch\n","!pip install scipy\n","!pip install transformers\n","!pip install sklearn"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"D0m5MYZ57PkP"},"outputs":[],"source":["# a copy from run_classfier to test tokenizer\n","\n","\n","# coding=utf-8\n","# Copyright 2018 The Google AI Language Team Authors and The HuggingFace Inc. team.\n","# Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.\n","#\n","# Licensed under the Apache License, Version 2.0 (the \"License\");\n","# you may not use this file except in compliance with the License.\n","# You may obtain a copy of the License at\n","#\n","#     http://www.apache.org/licenses/LICENSE-2.0\n","#\n","# Unless required by applicable law or agreed to in writing, software\n","# distributed under the License is distributed on an \"AS IS\" BASIS,\n","# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","# See the License for the specific language governing permissions and\n","# limitations under the License.\n","\"\"\"BERT finetuning runner.\"\"\"\n","\n","from __future__ import absolute_import, division, print_function\n","\n","import pickle\n","import argparse\n","import csv\n","import logging\n","import os\n","import random\n","import sys\n","import json\n","\n","import pandas as pd\n","import numpy as np\n","import torch\n","from torch.utils.data import (DataLoader, RandomSampler, SequentialSampler, SubsetRandomSampler, TensorDataset, ConcatDataset, Subset)\n","from torch.utils.data.distributed import DistributedSampler\n","from tqdm import trange\n","from tqdm.notebook import tqdm\n","\n","import torch\n","import torch.nn as nn\n","from torch.nn import CrossEntropyLoss, MSELoss, BCELoss\n","from scipy.stats import pearsonr, spearmanr\n","from sklearn.metrics import matthews_corrcoef,accuracy_score, f1_score\n","from sklearn.model_selection import KFold\n","\n","import transformers\n","from transformers import RobertaForSequenceClassification, RobertaConfig, BertTokenizer\n","from transformers import PYTORCH_PRETRAINED_BERT_CACHE, WEIGHTS_NAME, CONFIG_NAME\n","from transformers import AdamW, get_linear_schedule_with_warmup, get_constant_schedule\n","from transformers import AutoModel, AutoTokenizer, AutoModelForTokenClassification, AutoModelForSequenceClassification\n","from transformers import RobertaModel\n","\n","logger = logging.getLogger(__name__)\n"]},{"cell_type":"markdown","metadata":{"id":"52Oa2DSLKyYY"},"source":["# Metric eval"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"trrRCG7v8tCI"},"outputs":[],"source":["\n","# METRICS EVAL\n","def acc_and_f1(preds, labels):\n","    acc = accuracy_score(y_true=labels, y_pred=preds)\n","    f1 = f1_score(y_true=labels, y_pred=preds, average=\"macro\")\n","    return {\n","        \"acc\": acc,\n","        \"f1\": f1,\n","        \"acc_and_f1\": (acc + f1) / 2,\n","    }\n","\n","def pearson_and_spearman(preds, labels):\n","    pearson_corr = pearsonr(preds, labels)[0]\n","    spearman_corr = spearmanr(preds, labels)[0]\n","    return {\n","        \"pearson\": pearson_corr,\n","        \"spearmanr\": spearman_corr,\n","        \"corr\": (pearson_corr + spearman_corr) / 2,\n","    }\n"]},{"cell_type":"markdown","metadata":{"id":"v02r9GejKtp2"},"source":["# Arguments"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jIA_M9fj7PkV"},"outputs":[],"source":["class Arguments:\n","    def __init__(self):\n","        self.data_dir = \"/content/drive/MyDrive/pRoBERTa/data/data\"\n","        self.bert_model = \"vinai/phobert-base-v2\"\n","        self.task_name = \"fever\"\n","        self.output_dir = \"/content/drive/MyDrive/pRoBERTa/trained_module/rationale_selection/self_explain_model\"\n","        self.evidence_path = \"/content/drive/MyDrive/pRoBERTa/trained_module/rationale_selection/self_explain_model/pytorch_model.bin\"\n","        self.kfold = 10\n","        self.scale_pos_neg = 5\n","        self.cache_dir = \"\"\n","        self.max_seq_length = 128\n","        self.do_train = True\n","        self.do_eval = True\n","        self.do_test = False\n","        self.do_lower_case = False\n","        self.train_batch_size = 2\n","        self.negative_batch_size = 4\n","        self.losstype = 'cross_entropy_mining'\n","        self.eval_batch_size = 6\n","        self.learning_rate = 2e-5\n","        self.num_train_epochs = 5.0\n","        self.warmup_proportion = 0.1\n","        self.no_cuda = False\n","        self.local_rank = -1\n","        self.seed = 42\n","        self.gradient_accumulation_steps = 1\n","        self.fp16 = False\n","        self.loss_scale = 0.0\n","args=Arguments()\n"]},{"cell_type":"markdown","metadata":{"id":"eC9jXFkYKqjJ"},"source":["# Model\n"]},{"cell_type":"markdown","metadata":{"id":"kIgTWvOeK6pG"},"source":["## Colate function\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"W9nKMzNzK9co"},"outputs":[],"source":["from typing import List\n","\n","import numpy as np\n","import torch\n","\n","def collate_to_max_length(batch: List[List[torch.Tensor]], max_len: int = 128, fill_values: List[float] = None) -> \\\n","    List[torch.Tensor]:\n","    \"\"\"\n","    pad to maximum length of this batch\n","    Args:\n","        batch: a batch of samples, each contains a list of field data(Tensor), which shape is [seq_length]\n","        max_len: specify max length\n","        fill_values: specify filled values of each field\n","    Returns:\n","        output: list of field batched data, which shape is [batch, max_length]\n","    \"\"\"\n","    # [batch, num_fields]\n","    lengths = np.array([[len(field_data) for field_data in sample] for sample in batch])\n","    batch_size, num_fields = lengths.shape\n","    fill_values = fill_values or [0.0] * num_fields\n","    # [num_fields]\n","    max_lengths = max_len\n","    # if max_len:\n","    #     assert max_lengths.max() <= max_len\n","    #     max_lengths = np.ones_like(max_lengths) * max_len\n","\n","    output = [torch.full([batch_size, max_lengths],\n","                         fill_value=fill_values[field_idx],\n","                         dtype=batch[0][field_idx].dtype)\n","              for field_idx in range(num_fields)]\n","    for sample_idx in range(batch_size):\n","        for field_idx in range(num_fields):\n","            # seq_length\n","            data = batch[sample_idx][field_idx]\n","            output[field_idx][sample_idx][: data.shape[0]] = data\n","    # generate span_index and span_mask\n","    max_sentence_length = max_lengths\n","    start_indexs = []\n","    end_indexs = []\n","    for i in range(1, max_sentence_length - 1):\n","        for j in range(i, max_sentence_length - 1):\n","            # # span大小为10\n","            # if j - i > 10:\n","            #     continue\n","            start_indexs.append(i)\n","            end_indexs.append(j)\n","    # generate span mask\n","    span_masks = []\n","    for input_ids, label, length in batch:\n","        span_mask = []\n","        middle_index = input_ids.tolist().index(2)\n","        for start_index, end_index in zip(start_indexs, end_indexs):\n","            if 1 <= start_index <= length.item() - 2 and 1 <= end_index <= length.item() - 2 and (\n","                start_index > middle_index or end_index < middle_index):\n","                span_mask.append(0)\n","            else:\n","                span_mask.append(1e6)\n","        span_masks.append(span_mask)\n","    # add to output\n","    output.append(torch.LongTensor(start_indexs))\n","    output.append(torch.LongTensor(end_indexs))\n","    output.append(torch.LongTensor(span_masks))\n","    return output  # (input_ids, labels, length, start_indexs, end_indexs, span_masks)\n","\n"]},{"cell_type":"markdown","metadata":{"id":"nt_u7g2mPJQJ"},"source":["## DataLoader\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AycP_7XHPNPW"},"outputs":[],"source":["import json\n","import os\n","from functools import partial\n","\n","import torch\n","from torch.utils.data import Dataset, DataLoader\n","from transformers import RobertaTokenizer\n","\n","class DSC_Dataset(Dataset):\n","    def __init__(self, directory, prefix, bert_path, max_length: int = 128):\n","        super().__init__()\n","        self.max_length = max_length\n","        # label_map = {\"NON\": 0, 'SR': 1}\n","        with open(os.path.join(directory, prefix + '.json')) as file:\n","            data = json.load(file)\n","        self.result = data\n","        self.tokenizer = AutoTokenizer.from_pretrained(args.bert_model)\n","\n","    def __len__(self):\n","        return len(self.result)\n","\n","    def __getitem__(self, idx):\n","        # sentence_1, sentence_2, label = self.result[idx]\n","        # check for required keys\n","        if not all(key in self.result[idx] for key in [\"claim\", \"sentence\", \"evidence_label\"]):\n","            raise KeyError(\"Missing key in result dictionary.\")\n","\n","        sentence_1 = self.result[idx][\"claim\"]\n","        sentence_2 = self.result[idx][\"sentence\"]\n","        label = self.result[idx][\"evidence_label\"]\n","        # remove .\n","        if sentence_1.endswith(\".\"):\n","            sentence_1 = sentence_1[:-1]\n","        if sentence_2.endswith(\".\"):\n","            sentence_2 = sentence_2[:-1]\n","        sentence_1_input_ids = self.tokenizer.encode(\n","            sentence_1, add_special_tokens=False)\n","        sentence_2_input_ids = self.tokenizer.encode(\n","            sentence_2, add_special_tokens=False)\n","        input_ids = sentence_1_input_ids + [2] + sentence_2_input_ids\n","        if len(input_ids) > self.max_length - 2:\n","            input_ids = input_ids[:self.max_length - 2]\n","        # convert list to tensor\n","        length = torch.LongTensor([len(input_ids) + 2])\n","        input_ids = torch.LongTensor([0] + input_ids + [2])\n","        label = torch.LongTensor([label])\n","        return input_ids, label, length\n","\n","\n","# def unit_test():\n","#     root_path = \"D:\\\\DUCBUI\\\\DSC\\\\self_explanining\\\\data\\\\\"\n","#     bert_path = \"roberta-base\"\n","#     prefix = \"train_evidence\"\n","#     dataset = DSC_Dataset(directory=root_path,\n","#                           prefix=prefix, bert_path=bert_path)\n","\n","#     print(dataset)\n","\n","\n","\n","# if __name__ == '__main__':\n","#     unit_test()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aZk1Gn4r5wnJ"},"outputs":[],"source":["def get_feature(dataset, sampler, batch_size ) -> DataLoader:\n","    dataloader = DataLoader(\n","        dataset=dataset,\n","        sampler=sampler,\n","        batch_size=batch_size,\n","        collate_fn=partial(collate_to_max_length, fill_values=[1, 0, 0]),\n","        drop_last=False\n","    )\n","    return dataloader\n","\n","    # iterator = iter(dataloader)\n","    # batch = next(iterator)\n","    # input_ids, label, length, start_index, end_index, span_mask = batch\n","    # return input_ids, label, length, start_index, end_index, span_mask"]},{"cell_type":"markdown","metadata":{"id":"aHgzbxL05wnJ"},"source":["## Explainable model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"y3jE2CxKSKP3"},"outputs":[],"source":["class ExplainableModel(nn.Module):\n","    def __init__(self, bert_dir):\n","        super().__init__()\n","        self.bert_config = RobertaConfig.from_pretrained(\n","            bert_dir, output_hidden_states=True)\n","        self.intermediate = RobertaModel.from_pretrained(bert_dir)\n","        self.span_info_collect = SICModel(self.bert_config.hidden_size)\n","        self.interpretation = InterpretationModel(self.bert_config.hidden_size)\n","        self.output = nn.Linear(\n","            self.bert_config.hidden_size, self.bert_config.num_labels)\n","\n","    def forward(self, input_ids, start_indexs, end_indexs, span_masks):\n","        # generate mask\n","        attention_mask = (input_ids != 1).long()\n","        # intermediate layer\n","        a = self.intermediate(\n","            input_ids, attention_mask=attention_mask)\n","        hidden_states = self.intermediate(\n","            input_ids, attention_mask=attention_mask).last_hidden_state\n","        # span info collecting layer(SIC)\n","        h_ij = self.span_info_collect(hidden_states, start_indexs, end_indexs)\n","        # interpretation layer\n","        H, a_ij = self.interpretation(h_ij, span_masks)\n","        # output layer\n","        out = self.output(H)\n","        return out, a_ij\n","\n","\n","class SICModel(nn.Module):\n","    def __init__(self, hidden_size):\n","        super().__init__()\n","        self.hidden_size = hidden_size\n","\n","        self.W_1 = nn.Linear(hidden_size, hidden_size)\n","        self.W_2 = nn.Linear(hidden_size, hidden_size)\n","        self.W_3 = nn.Linear(hidden_size, hidden_size)\n","        self.W_4 = nn.Linear(hidden_size, hidden_size)\n","\n","    def forward(self, hidden_states, start_indexs, end_indexs):\n","        W1_h = self.W_1(hidden_states)  # (bs, length, hidden_size)\n","        W2_h = self.W_2(hidden_states)\n","        W3_h = self.W_3(hidden_states)\n","        W4_h = self.W_4(hidden_states)\n","\n","        # (bs, span_num, hidden_size)\n","        W1_hi_emb = torch.index_select(W1_h, 1, start_indexs)\n","        W2_hj_emb = torch.index_select(W2_h, 1, end_indexs)\n","        W3_hi_start_emb = torch.index_select(W3_h, 1, start_indexs)\n","        W3_hi_end_emb = torch.index_select(W3_h, 1, end_indexs)\n","        W4_hj_start_emb = torch.index_select(W4_h, 1, start_indexs)\n","        W4_hj_end_emb = torch.index_select(W4_h, 1, end_indexs)\n","\n","        # [w1*hi, w2*hj, w3(hi-hj), w4(hi⊗hj)]\n","        span = W1_hi_emb + W2_hj_emb + \\\n","            (W3_hi_start_emb - W3_hi_end_emb) + \\\n","            torch.mul(W4_hj_start_emb, W4_hj_end_emb)\n","        h_ij = torch.tanh(span)\n","        return h_ij\n","\n","\n","class InterpretationModel(nn.Module):\n","    def __init__(self, hidden_size):\n","        super().__init__()\n","        self.h_t = nn.Linear(hidden_size, 1)\n","\n","    def forward(self, h_ij, span_masks):\n","        o_ij = self.h_t(h_ij).squeeze(-1)  # (ba, span_num)\n","        # mask illegal span\n","        o_ij = o_ij - span_masks\n","        # normalize all a_ij, a_ij sum = 1\n","        a_ij = nn.functional.softmax(o_ij, dim=1)\n","        # weight average span representation to get H\n","        H = (a_ij.unsqueeze(-1) * h_ij).sum(dim=1)  # (bs, hidden_size)\n","        return H, a_ij\n","\n","\n","# def main():\n","#     # data\n","#     input_id_1 = torch.LongTensor([0, 4, 5, 6, 7, 2])\n","#     input_id_2 = torch.LongTensor([0, 4, 5, 2])\n","#     input_id_3 = torch.LongTensor([0, 4, 2])\n","#     batch = [(input_id_1, torch.LongTensor([1]), torch.LongTensor([6])),\n","#              (input_id_2, torch.LongTensor([1]), torch.LongTensor([4])),\n","#              (input_id_3, torch.LongTensor([1]), torch.LongTensor([3]))]\n","\n","#     output = collate_to_max_length(batch=batch, fill_values=[1, 0, 0])\n","#     input_ids, labels, length, start_indexs, end_indexs, span_masks = output\n","\n","#     # model\n","#     bert_path = \"roberta-base\"\n","#     model = ExplainableModel(bert_path)\n","#     print(model)\n","\n","#     output = model(input_ids, start_indexs, end_indexs, span_masks)\n","#     print(output[0])\n","\n","\n","# if __name__ == '__main__':\n","#     main()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kMWoglmSKONX","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1698943823296,"user_tz":-420,"elapsed":3079,"user":{"displayName":"Anh Đức Bùi","userId":"02440975349884324274"}},"outputId":"a1a478e0-c451-4058-912a-5810fb929025"},"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of RobertaModel were not initialized from the model checkpoint at vinai/phobert-base-v2 and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}],"source":["model = ExplainableModel(args.bert_model)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WkRt7BMX5wnK","executionInfo":{"status":"ok","timestamp":1698943823297,"user_tz":-420,"elapsed":10,"user":{"displayName":"Anh Đức Bùi","userId":"02440975349884324274"}},"outputId":"2cd4d00e-1584-4452-e423-4b519a473de0"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["ExplainableModel(\n","  (intermediate): RobertaModel(\n","    (embeddings): RobertaEmbeddings(\n","      (word_embeddings): Embedding(64001, 768, padding_idx=1)\n","      (position_embeddings): Embedding(258, 768, padding_idx=1)\n","      (token_type_embeddings): Embedding(1, 768)\n","      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (encoder): RobertaEncoder(\n","      (layer): ModuleList(\n","        (0-11): 12 x RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","    (pooler): RobertaPooler(\n","      (dense): Linear(in_features=768, out_features=768, bias=True)\n","      (activation): Tanh()\n","    )\n","  )\n","  (span_info_collect): SICModel(\n","    (W_1): Linear(in_features=768, out_features=768, bias=True)\n","    (W_2): Linear(in_features=768, out_features=768, bias=True)\n","    (W_3): Linear(in_features=768, out_features=768, bias=True)\n","    (W_4): Linear(in_features=768, out_features=768, bias=True)\n","  )\n","  (interpretation): InterpretationModel(\n","    (h_t): Linear(in_features=768, out_features=1, bias=True)\n","  )\n","  (output): Linear(in_features=768, out_features=2, bias=True)\n",")"]},"metadata":{},"execution_count":11}],"source":["model"]},{"cell_type":"markdown","metadata":{"id":"ZfihoyRnSHxz"},"source":["# Train"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":533,"referenced_widgets":["16800c7713694f9cbd9bd90c151a2795","891daeece03a439a89fd4023f254995e","a4be7a3d5ae54c0ca1a460b3fecbf576","be7e5ad7e4704782bcf68184a11ccb6b","10c3d897043d4ac1b55003c7746a4735","c806d4201adc48f09ee2c03c1f86288b","2d38aa9ad6ea44e2808535816ad5f4d9","2209b930e7b14a499cdb70e7e37a7859","0de110103e474b5490c8ee1bc668b9e7","8c279245d7a14bff83cf880036f3015a","a7b0106b19b74affaae697fd139decdd"]},"id":"J17U29Ly7PkV","outputId":"e063f9be-ba8c-4b45-a2cc-cd31f04e5341","executionInfo":{"status":"error","timestamp":1698943840150,"user_tz":-420,"elapsed":16859,"user":{"displayName":"Anh Đức Bùi","userId":"02440975349884324274"}}},"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of RobertaModel were not initialized from the model checkpoint at vinai/phobert-base-v2 and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n","Epoch:   0%|          | 0/5 [00:00<?, ?it/s]"]},{"output_type":"display_data","data":{"text/plain":["Iteration:   0%|          | 0/6548 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"16800c7713694f9cbd9bd90c151a2795"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["\rEpoch:   0%|          | 0/5 [00:05<?, ?it/s]\n"]},{"output_type":"error","ename":"RuntimeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-12-5c9e6bc52584>\u001b[0m in \u001b[0;36m<cell line: 371>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    370\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    371\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 372\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-12-5c9e6bc52584>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    187\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 189\u001b[0;31m                     \u001b[0moutput_logits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids_cat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_indexs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstart_index_cat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_indexs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mend_index_cat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspan_masks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mspan_mask_cat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0moutput_mode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"classification\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-9-d381b4ac37c7>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, start_indexs, end_indexs, span_masks)\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mh_ij\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspan_info_collect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_indexs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_indexs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;31m# interpretation layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0mH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma_ij\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterpretation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh_ij\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspan_masks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[0;31m# output layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-9-d381b4ac37c7>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, h_ij, span_masks)\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0mo_ij\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mh_t\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh_ij\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# (ba, span_num)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0;31m# mask illegal span\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m         \u001b[0mo_ij\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mo_ij\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mspan_masks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m         \u001b[0;31m# normalize all a_ij, a_ij sum = 1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0ma_ij\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo_ij\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (16002) must match the size of tensor b (8001) at non-singleton dimension 1"]}],"source":["def main():\n","\n","    # processors = {\n","    #     \"fever\": FeverProcessor,\n","    # }\n","\n","    output_modes = {\n","        \"fever\": \"classification\",\n","    }\n","\n","    # Huấn luyện song song trên GPU\n","    if args.local_rank == -1 or args.no_cuda:\n","        device = torch.device(\"cuda\" if torch.cuda.is_available() and not args.no_cuda else \"cpu\")\n","        n_gpu = torch.cuda.device_count()\n","    else:\n","        torch.cuda.set_device(args.local_rank)\n","        device = torch.device(\"cuda\", args.local_rank)\n","        n_gpu = 1\n","        # Initializes the distributed backend which will take care of sychronizing nodes/GPUs\n","        torch.distributed.init_process_group(backend='nccl')\n","\n","    # Xuất thông báo khởi tạo môi trường cho việc huấn luyện\n","    logging.basicConfig(format='%(asctime)s - %(levelname)s - %(name)s -   %(message)s',\n","                        datefmt='%m/%d/%Y %H:%M:%S',\n","                        level=logging.INFO if args.local_rank in [-1, 0] else logging.WARN)\n","\n","    logger.info(\"device: {} n_gpu: {}, distributed training: {}, 16-bits training: {}\".format(\n","        device, n_gpu, bool(args.local_rank != -1), args.fp16))\n","\n","    if args.gradient_accumulation_steps < 1:\n","        raise ValueError(\"Invalid gradient_accumulation_steps parameter: {}, should be >= 1\".format(\n","            args.gradient_accumulation_steps))\n","\n","    args.train_batch_size = args.train_batch_size // args.gradient_accumulation_steps\n","\n","    random.seed(args.seed)\n","    np.random.seed(args.seed)\n","    torch.manual_seed(args.seed)\n","    if n_gpu > 0:\n","        torch.cuda.manual_seed_all(args.seed)\n","\n","    if not args.do_train and not args.do_eval:\n","        raise ValueError(\"At least one of `do_train` or `do_eval` must be True.\")\n","\n","    if os.path.exists(args.output_dir) and os.listdir(args.output_dir) and args.do_train:\n","        raise ValueError(\"Output directory ({}) already exists and is not empty.\".format(args.output_dir))\n","\n","    if not os.path.exists(args.output_dir):\n","        os.makedirs(args.output_dir)\n","\n","    task_name = args.task_name.lower()\n","\n","    # if task_name not in processors:\n","    #     raise ValueError(\"Task not found: %s\" % (task_name))\n","\n","    # processor = processors[task_name]()\n","    output_mode = output_modes[task_name]\n","\n","    label_list = [\"NON\", \"SR\"]\n","    num_labels = len(label_list)\n","\n","    tokenizer = AutoTokenizer.from_pretrained(args.bert_model, use_fast=False)\n","    # tokenizer = AutoTokenizer.from_pretrained(args.bert_model, do_lower_case=args.do_lower_case)\n","\n","    train_examples = None\n","    num_train_optimization_steps = None\n","\n","    # Prepare model\n","    cache_dir = args.cache_dir if args.cache_dir else os.path.join(str(PYTORCH_PRETRAINED_BERT_CACHE),\n","                                                                   'distributed_{}'.format(args.local_rank))\n","    model = ExplainableModel(args.bert_model)\n","\n","    # DEBUG\n","    # model.load_state_dict(torch.load(args.evidence_path))\n","    # model.cuda()\n","\n","    if args.fp16:\n","        model.half()\n","    model.to(device)\n","    if args.local_rank != -1:\n","        try:\n","            from apex.parallel import DistributedDataParallel as DDP\n","        except ImportError:\n","            raise ImportError(\n","                \"Please install apex from https://www.github.com/nvidia/apex to use distributed and fp16 training.\")\n","\n","        model = DDP(model)\n","    elif n_gpu > 1:\n","        model = torch.nn.DataParallel(model)\n","\n","    # Prepare optimizer\n","    param_optimizer = list(model.named_parameters())\n","    no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n","    optimizer_grouped_parameters = [\n","        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n","        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n","    ]\n","\n","    optimizer = AdamW(optimizer_grouped_parameters,\n","                        lr=args.learning_rate)\n","\n","    global_step = 0\n","    nb_tr_steps = 0\n","    tr_loss = 0\n","\n","    logger.info(f\"Optimzer:{optimizer.__class__.__name__}\")\n","\n","    if args.do_train:\n","        kfold = KFold(n_splits=args.kfold, shuffle=True, random_state=42)\n","\n","        train_dataset_pos = DSC_Dataset(directory=args.data_dir, prefix=\"train_pos\", bert_path=args.bert_model, max_length=args.max_seq_length)\n","\n","        train_dataset_neg = DSC_Dataset(directory=args.data_dir, prefix=\"train_neg\", bert_path=args.bert_model, max_length=args.max_seq_length)\n","\n","        num_train_optimization_steps = int(\n","            len(train_dataset_pos) / args.train_batch_size / args.gradient_accumulation_steps) * args.num_train_epochs\n","\n","        # logger.info(f\"\\nNum Train Optimization Steps: {num_train_optimization_steps}\\n\")\n","\n","        if args.losstype == 'cross_entropy_concat' or args.losstype == 'hinge_loss_concat':\n","            num_train_optimization_steps = int((len(train_dataset_pos)+len(train_dataset_neg)) / args.train_batch_size / args.gradient_accumulation_steps) * args.num_train_epochs\n","\n","        if args.local_rank != -1:\n","            num_train_optimization_steps = num_train_optimization_steps // torch.distributed.get_world_size()\n","\n","\n","        logger.info(\"***** Running training *****\")\n","        logger.info(\"  Num examples = %d\", len(train_dataset_pos))\n","        logger.info(\"  Batch size = %d\", args.train_batch_size)\n","        logger.info(\"  Num steps = %d\", num_train_optimization_steps)\n","\n","        pos_fold = {\"train\": [], \"test\": []}\n","        for fold_train, fold_test in kfold.split(train_dataset_pos):\n","            pos_fold[\"train\"].append(fold_train)\n","            pos_fold[\"test\"].append(fold_test)\n","\n","        neg_fold = {\"train\": [], \"test\": []}\n","        for fold_train, fold_test in kfold.split(train_dataset_neg):\n","            neg_fold[\"train\"].append(fold_train)\n","            neg_fold[\"test\"].append(fold_test)\n","\n","\n","        for epoch in trange(int(args.num_train_epochs), desc=\"Epoch\"):\n","            model.train()\n","            label_pred = []\n","            label_true = []\n","            tr_loss = 0\n","            nb_tr_examples, nb_tr_steps = 0, 0\n","            idx_train_pos = pos_fold[\"train\"][epoch]\n","            idx_train_neg = neg_fold[\"train\"][epoch]\n","            idx_eval_pos = pos_fold[\"test\"][epoch]\n","            idx_eval_neg = neg_fold[\"test\"][epoch]\n","\n","            train_sampler_pos = SubsetRandomSampler(idx_train_pos)\n","            train_sampler_neg = SubsetRandomSampler(idx_train_neg)\n","\n","            # train_dataloader_pos = DataLoader(train_data_pos, sampler=train_sampler_pos, batch_size=args.train_batch_size)\n","            # train_dataloader_neg = DataLoader(train_data_neg, sampler=train_sampler_neg, batch_size=args.negative_batch_size)\n","\n","            train_dataloader_pos = get_feature(dataset=train_dataset_pos,sampler=train_sampler_pos, batch_size = args.train_batch_size)\n","            train_dataloader_neg = get_feature(dataset=train_dataset_neg,sampler=train_sampler_neg, batch_size = args.train_batch_size)\n","\n","\n","            # eval_sampler = SubsetRandomSampler(pos_fold[\"test\"][epoch])\n","            # eval_neg_sampler = SubsetRandomSampler(neg_fold[\"test\"][epoch])\n","            eval_data = ConcatDataset([Subset(train_dataset_pos,idx_eval_pos),Subset(train_dataset_neg, idx_eval_neg)])\n","            eval_sampler = RandomSampler(eval_data)\n","            eval_dataloader = get_feature(dataset=eval_data,sampler=eval_sampler, batch_size = args.eval_batch_size)\n","            it=iter(train_dataloader_neg)\n","            for step, batch in enumerate(tqdm(train_dataloader_pos, desc=\"Iteration\")):\n","                batch = tuple(t.to(device) for t in batch)\n","                input_ids, label, length, start_index, end_index, span_mask = batch\n","\n","                # define a new function to compute loss values for both output_modes\n","                try:\n","                    batch_neg = tuple(t.to(device) for t in next(it))\n","                except:\n","                    it = iter(train_dataloader_neg)\n","                    batch_neg = tuple(t.to(device) for t in next(it))\n","                input_ids_neg, label_neg, length_neg, start_index_neg, end_index_neg, span_mask_neg = batch_neg\n","\n","                input_ids_cat=torch.cat([input_ids, input_ids_neg],dim=0)\n","                start_index_cat=torch.cat([start_index, start_index_neg],dim=0)\n","                end_index_cat=torch.cat([end_index,end_index_neg],dim=0)\n","                span_mask_cat=torch.cat([span_mask,span_mask_neg],dim=0)\n","                label_cat=torch.cat([label.view(-1), label_neg.view(-1)], dim = 0)\n","\n","                with torch.no_grad():\n","                    output_logits = model(input_ids=input_ids_cat, start_indexs=start_index_cat, end_indexs=end_index_cat, span_masks=span_mask_cat)\n","\n","                if output_mode == \"classification\":\n","                    loss_fct = CrossEntropyLoss()\n","                    if args.losstype == 'cross_entropy':\n","                        loss = loss_fct(output_logits.logits.view(-1, num_labels),label_cat)\n","\n","                    elif args.losstype == 'cross_entropy_mining':\n","\n","                        loss_fct = CrossEntropyLoss(reduction='none')\n","                        loss = loss_fct(output_logits.logits.view(-1, num_labels),label_cat)\n","                        NON_index = label_cat.view(-1) == 1\n","                        SR_index = label_cat.view(-1) != 1\n","                        Pos_numbers = SR_index.sum()\n","                        Top_numbers = Pos_numbers * args.scale_pos_neg\n","\n","                        TOP_K, Hard_NON_index = loss[NON_index].topk(Top_numbers)\n","                        # loss = torch.cat([loss[SR_index], TOP_K])    #uncomment if dont want to use no-grad then grad\n","\n","                        #comment if dont want to use no-grad then grad\n","                        IDS = torch.cat([torch.tensor(range(0,Pos_numbers),device=device), Pos_numbers+Hard_NON_index], dim=0).to(torch.int32)\n","                        output_logits = model(input_ids=input_ids_cat[IDS, :],start_indexs=start_index_cat[IDS, :], end_indexs=end_index_cat[IDS, :], span_masks=span_mask_neg[IDS, :])\n","\n","                        # Compute loss, evaluation\n","                        probs = torch.nn.functional.softmax(output_logits.logits, dim=-1)\n","                        _, pred = torch.max(probs, dim=-1)\n","\n","                        label_pred.extend(pred.cpu().numpy())\n","                        label_true.extend(label_cat[IDS].cpu().numpy())\n","\n","                        # loss_fct = CrossEntropyLoss()\n","                        loss_fct = CrossEntropyLoss(weight=torch.tensor([args.scale_pos_neg,1.]).to(device))\n","                        loss = loss_fct(output_logits.logits.view(-1, num_labels), label_cat[IDS])\n","\n","                elif output_mode == \"regression\":\n","                    loss_fct = MSELoss()\n","                    loss = loss_fct(output_logits.logits.view(-1), label_cat)\n","\n","                if n_gpu > 1:\n","                    loss = loss.mean()  # mean() to average on multi-gpu.\n","                if args.gradient_accumulation_steps > 1:\n","                    loss = loss / args.gradient_accumulation_steps\n","\n","                if args.fp16:\n","                    optimizer.backward(loss)\n","                else:\n","                    loss.backward()\n","\n","                tr_loss += loss.item()\n","                nb_tr_examples += input_ids.size(0)\n","                nb_tr_steps += 1\n","\n","                if (step + 1) % args.gradient_accumulation_steps == 0:\n","                    optimizer.step()\n","                    optimizer.zero_grad()\n","                    global_step += 1\n","            print('printing loss')\n","            print('training_loss~=',tr_loss/nb_tr_steps)\n","            print(acc_and_f1(preds=label_pred, labels=label_true))\n","\n","\n","            if args.do_train and (args.local_rank == -1 or torch.distributed.get_rank() == 0):\n","                # Save a trained model, configuration and tokenizer\n","                model_to_save = model.module if hasattr(model, 'module') else model  # Only save the model it-self\n","\n","                # If we save using the predefined names, we can load using `from_pretrained`\n","                output_model_file = os.path.join(args.output_dir, f\"{WEIGHTS_NAME}_{epoch}\")\n","                output_config_file = os.path.join(args.output_dir, CONFIG_NAME)\n","\n","                torch.save(model_to_save.state_dict(), output_model_file)\n","                if epoch == 0:\n","                    tokenizer.save_vocabulary(args.output_dir)\n","\n","            model.eval()\n","            eval_loss = 0\n","            nb_eval_steps = 0\n","            preds = []\n","            preds = []\n","            labels = []\n","            probs = []\n","            store_output=list()\n","            # softmaxing=torch.nn.Softmax()\n","            for input_ids, label, length, start_index, end_index, span_mask in tqdm(eval_dataloader, desc=\"Evaluating\"):\n","                input_ids = input_ids.to(device)\n","                start_index = start_index.to(device)\n","                end_index = end_index.to(device)\n","                span_mask = span_mask.to(device)\n","                label_ids = label.to(device)\n","                #\n","                model.zero_grad()\n","                with torch.no_grad():\n","                    # output_logits = model(input_ids, segment_ids, input_mask, labels=None)\n","                    output_logits = model(input_ids=input_ids_cat, start_indexs=start_index_cat, end_indexs=end_index_cat, span_masks=span_mask_cat)\n","\n","                    # logits=softmaxing(logits)\n","                    store_output.extend(output_logits.logits.cpu().numpy())\n","                    prob_in_batch = torch.nn.functional.softmax(output_logits.logits, dim=-1)\n","\n","                    # Chọn nhãn với xác suất cao nhất\n","                    prob, pred = torch.max(prob_in_batch, dim=-1)\n","                    preds.extend(pred.cpu().numpy())\n","                    labels.extend(label_ids.cpu().numpy())\n","                    probs.extend(prob.cpu().numpy())\n","            print(acc_and_f1(preds=preds, labels=labels))\n","\n","            # if args.do_train and (args.local_rank == -1 or torch.distributed.get_rank() == 0):\n","            #     # Save a trained model, configuration and tokenizer\n","            #     model_to_save = model.module if hasattr(model, 'module') else model  # Only save the model it-self\n","\n","            #     # If we save using the predefined names, we can load using `from_pretrained`\n","            #     output_model_file = os.path.join(args.output_dir, f\"{WEIGHTS_NAME}_{epoch}\")\n","            #     output_config_file = os.path.join(args.output_dir, CONFIG_NAME)\n","\n","            #     torch.save(model_to_save.state_dict(), output_model_file)\n","            #     if epoch == 0:\n","            #         tokenizer.save_vocabulary(args.output_dir)\n","\n","\n","    # if args.do_test and (args.local_rank == -1 or torch.distributed.get_rank() == 0):\n","    #   # model.load_state_dict(torch.load(os.path.join(args.output_dir,f\"pytorch_model_{epoch}.bin\")))\n","    #     test_examples = processor.get_test_examples(args.data_dir)\n","    #     # test_examples=test_examples[0:500] #debugging\n","    #     test_features = convert_examples_to_features_test(\n","    #         test_examples, label_list, args.max_seq_length, tokenizer, output_mode)\n","    #     logger.info(\"***** Running testing *****\")\n","    #     logger.info(\"  Num examples = %d\", len(test_examples))\n","    #     logger.info(\"  Batch size = %d\", args.eval_batch_size)\n","    #     all_input_ids = torch.tensor([f.input_ids for f in test_features], dtype=torch.long)\n","    #     all_input_mask = torch.tensor([f.input_mask for f in test_features], dtype=torch.long)\n","    #     all_segment_ids = torch.tensor([f.segment_ids for f in test_features], dtype=torch.long)\n","\n","    #     test_data = TensorDataset(all_input_ids, all_input_mask, all_segment_ids)\n","    #     # Run prediction for full data\n","    #     test_sampler = SequentialSampler(test_data)\n","    #     test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=args.eval_batch_size)\n","\n","    #     model.eval()\n","    #     preds = []\n","    #     probs = []\n","    #     store_output=list()\n","    #     # softmaxing=torch.nn.Softmax()\n","    #     for input_ids, input_mask, segment_ids in tqdm(test_dataloader, desc=\"Evaluating\"):\n","    #         input_ids = input_ids.to(device)\n","    #         input_mask = input_mask.to(device)\n","    #         segment_ids = segment_ids.to(device)\n","\n","    #         with torch.no_grad():\n","    #             output_logits = model(input_ids=input_ids, token_type_ids=None, attention_mask=input_mask, labels=None)\n","    #             store_output.extend(output_logits.logits.cpu().numpy())\n","    #             prob_in_batch = torch.nn.functional.softmax(output_logits.logits, dim=-1)\n","\n","    #             # Chọn nhãn với xác suất cao nhất\n","    #             prob, pred = torch.max(prob_in_batch, dim=-1)\n","    #             preds.extend(pred.cpu().numpy())\n","    #             probs.extend(prob.cpu().numpy())\n","    #     # torch.from_numpy(np.array(store_output))\n","    #     print(\"Storing dev scores\")\n","    #     claim_ids = []\n","    #     num_sentences = []\n","    #     claims =[]\n","    #     evidences = []\n","    #     ids = []\n","    #     for example in test_examples:\n","    #         id, claim_id, num_sentence, claim, evidence = processor.get_info_eval(example)\n","    #         ids.append(id)\n","    #         claim_ids.append(claim_id)\n","    #         num_sentences.append(num_sentence)\n","    #         claims.append(claim)\n","    #         evidences.append(evidence)\n","\n","    #     data = {\n","    #         \"claim_id\": claim_ids,\n","    #         \"num_sentence\": num_sentences,\n","    #         \"claim\": claims,\n","    #         \"evidence\": evidences,\n","    #         \"pred\": preds,\n","    #         \"prob\": probs\n","    #     }\n","    #     eval_df = pd.DataFrame(data=data)\n","    #     eval_df.to_json(os.path.join(args.output_dir,\"test_non_evidence.json\"), force_ascii=False, indent=4, orient='index')\n","\n","\n","if __name__ == \"__main__\":\n","    main()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BuSWxrGy5wnL"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.18"},"widgets":{"application/vnd.jupyter.widget-state+json":{"16800c7713694f9cbd9bd90c151a2795":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_891daeece03a439a89fd4023f254995e","IPY_MODEL_a4be7a3d5ae54c0ca1a460b3fecbf576","IPY_MODEL_be7e5ad7e4704782bcf68184a11ccb6b"],"layout":"IPY_MODEL_10c3d897043d4ac1b55003c7746a4735"}},"891daeece03a439a89fd4023f254995e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c806d4201adc48f09ee2c03c1f86288b","placeholder":"​","style":"IPY_MODEL_2d38aa9ad6ea44e2808535816ad5f4d9","value":"Iteration:   0%"}},"a4be7a3d5ae54c0ca1a460b3fecbf576":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"danger","description":"","description_tooltip":null,"layout":"IPY_MODEL_2209b930e7b14a499cdb70e7e37a7859","max":6548,"min":0,"orientation":"horizontal","style":"IPY_MODEL_0de110103e474b5490c8ee1bc668b9e7","value":0}},"be7e5ad7e4704782bcf68184a11ccb6b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8c279245d7a14bff83cf880036f3015a","placeholder":"​","style":"IPY_MODEL_a7b0106b19b74affaae697fd139decdd","value":" 0/6548 [00:05&lt;?, ?it/s]"}},"10c3d897043d4ac1b55003c7746a4735":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c806d4201adc48f09ee2c03c1f86288b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2d38aa9ad6ea44e2808535816ad5f4d9":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"2209b930e7b14a499cdb70e7e37a7859":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0de110103e474b5490c8ee1bc668b9e7":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"8c279245d7a14bff83cf880036f3015a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a7b0106b19b74affaae697fd139decdd":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}