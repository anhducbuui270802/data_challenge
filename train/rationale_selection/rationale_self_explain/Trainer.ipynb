{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":19825,"status":"ok","timestamp":1698905261694,"user":{"displayName":"Anh Đức Bùi","userId":"02440975349884324274"},"user_tz":-420},"id":"xt2n2W4O0L2N","outputId":"6ed53ac8-abb2-46f0-fa83-ff7598460dc9"},"outputs":[],"source":["# from google.colab import drive\n","# drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":21012,"status":"ok","timestamp":1698906083366,"user":{"displayName":"Anh Đức Bùi","userId":"02440975349884324274"},"user_tz":-420},"id":"t6iRGpu9Arfm","outputId":"a023862f-a7a8-45fd-838d-f2fa8bf8570a"},"outputs":[],"source":["# !pip install transformers\n","# !pip install pytorch_lightning"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":9515,"status":"ok","timestamp":1698906092877,"user":{"displayName":"Anh Đức Bùi","userId":"02440975349884324274"},"user_tz":-420},"id":"qozRZHlP2UWB"},"outputs":[],"source":["import argparse\n","import json\n","import os\n","from functools import partial\n","import pytorch_lightning as pl\n","import torch\n","from pytorch_lightning import Trainer\n","from pytorch_lightning.callbacks.model_checkpoint import ModelCheckpoint\n","from pytorch_lightning.loggers import TensorBoardLogger\n","from torch.nn import functional as F\n","from torch.nn.modules import CrossEntropyLoss\n","from torch.utils.data.dataloader import DataLoader\n","from transformers import AdamW, get_linear_schedule_with_warmup, RobertaTokenizer\n","from transformers import RobertaConfig, RobertaModel, AutoTokenizer, AutoModel\n","from torchmetrics import Accuracy\n","from typing import List\n","import numpy as np\n","from torch.utils.data import Dataset, DataLoader\n","import torch.nn as nn"]},{"cell_type":"markdown","metadata":{"id":"DKIndvJsv1yn"},"source":["# Args"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":14,"status":"ok","timestamp":1698906092878,"user":{"displayName":"Anh Đức Bùi","userId":"02440975349884324274"},"user_tz":-420},"id":"uburtJwLA-J9","outputId":"49071215-5a82-47f5-ebbd-7d7c62333205"},"outputs":[{"name":"stdout","output_type":"stream","text":["Using device \"cuda\"\n"]}],"source":["device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","print(f'Using device \"{device}\"')"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":13,"status":"ok","timestamp":1698906092878,"user":{"displayName":"Anh Đức Bùi","userId":"02440975349884324274"},"user_tz":-420},"id":"3wxhidvQyOtt"},"outputs":[],"source":["class parser():\n","  def __init__(self):\n","    self.bert_path = \"vinai/phobert-base-v2\"\n","    self.batch_size=8\n","    self.lr=2e-5\n","    self.workers=0\n","    self.weight_decay=0.0\n","    self.adam_epsilon=1e-9\n","    self.warmup_steps=0\n","    self.max_length=128\n","    self.data_dir=\"D:\\\\DUCBUI\\\\DSC\\\\self\\\\data\\\\converted_data_train_tocken.json\"\n","    self.save_path=\"D:\\\\DUCBUI\\\\DSC\\\\self\\\\save_model\\\\model\"\n","    self.save_topk=1\n","    self.checkpoint_path=\"D:\\\\DUCBUI\\\\DSC\\\\self\\\\save_model\\\\check_point\"\n","    self.span_topk=2\n","    self.lamb=1.0\n","    self.task='snli'\n","    self.mode='train'\n","    self.max_epochs=10\n","    self.accumulate_grad_batches=2\n","    self.scale_pos_neg = 5\n","\n","args = parser()"]},{"cell_type":"markdown","metadata":{"id":"ObWu8rUxv3lL"},"source":["# Prepare data\n"]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":1185,"status":"ok","timestamp":1698906094051,"user":{"displayName":"Anh Đức Bùi","userId":"02440975349884324274"},"user_tz":-420},"id":"7F8ZBZlYv6Ip"},"outputs":[],"source":["import random\n","from torch.utils.data import Dataset\n","import json\n","import pandas as pd\n","class LoadDataFromJson(Dataset, ):\n","    def __init__(self, datasets):\n","        self.samples = []\n","        with open(datasets) as file:\n","            data = json.load(file)\n","\n","        for id, item in data.items():\n","            sentences = item[\"context\"]\n","            if item[\"evidence\"] != None:\n","              for sentence in sentences:\n","                data_sample = {}\n","                evidence = item['evidence'] in sentence or sentence in item['evidence']\n","                if evidence:\n","                  data_sample['evidence_label'] = 1\n","                else:\n","                  data_sample['evidence_label'] = 0\n","                data_sample['id'] = id\n","                data_sample['claim'] = item['claim']\n","                data_sample['sentence'] = sentence\n","                data_sample['evidence'] = item['evidence']\n","\n","                data_sample[\"verdict\"] = item[\"verdict\"]\n","                data_sample['domain'] = item['domain']\n","                self.samples.append(data_sample)\n","\n","    def to_dataframe(self):\n","        return pd.DataFrame(self.samples).sample(frac=1, random_state=3)\n"]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":2637,"status":"ok","timestamp":1698906096686,"user":{"displayName":"Anh Đức Bùi","userId":"02440975349884324274"},"user_tz":-420},"id":"LQdTXxkPxFbF"},"outputs":[],"source":["a = LoadDataFromJson(args.data_dir)\n","df = a.to_dataframe()\n"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":580},"executionInfo":{"elapsed":8,"status":"ok","timestamp":1698906096686,"user":{"displayName":"Anh Đức Bùi","userId":"02440975349884324274"},"user_tz":-420},"id":"P9mhqh1hxbhR","outputId":"fc0d90ac-fa72-4477-893d-a792cdb46a7d"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>evidence_label</th>\n","      <th>id</th>\n","      <th>claim</th>\n","      <th>sentence</th>\n","      <th>evidence</th>\n","      <th>verdict</th>\n","      <th>domain</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>234309</th>\n","      <td>0</td>\n","      <td>30927</td>\n","      <td>khi gia_nhập college compass nhung được kết_nố...</td>\n","      <td>trong bài_luận gửi đến trường harvard nữ_sinh_...</td>\n","      <td>khi trở_thành học_sinh của college compass nhu...</td>\n","      <td>SUPPORTED</td>\n","      <td>giaoduc</td>\n","    </tr>\n","    <tr>\n","      <th>223745</th>\n","      <td>0</td>\n","      <td>30428</td>\n","      <td>từ lúc nghỉ_việc gia_đình chị thúy rất khó khắ...</td>\n","      <td>lê tuyết</td>\n","      <td>từ lúc nghỉ_việc gia_đình chị thúy chỉ kiếm đư...</td>\n","      <td>SUPPORTED</td>\n","      <td>thoisu</td>\n","    </tr>\n","    <tr>\n","      <th>94193</th>\n","      <td>0</td>\n","      <td>11558</td>\n","      <td>cần nhiều công_nghệ và thời_gian để khắc_phục ...</td>\n","      <td>khi tìm_kiếm sản_phẩm chăm_sóc da đặc_biệt cho...</td>\n","      <td>bác_sĩ thành cho rằng trường_hợp bệnh_nhân trê...</td>\n","      <td>SUPPORTED</td>\n","      <td>suckhoe</td>\n","    </tr>\n","    <tr>\n","      <th>114060</th>\n","      <td>0</td>\n","      <td>12370</td>\n","      <td>ông trump không_thể tự ân_xá cho mình dù trở_t...</td>\n","      <td>quá_trình xử_lý vụ án có_thể diễn ra trong thờ...</td>\n","      <td>nếu trở_thành tổng_thống ông trump không nắm g...</td>\n","      <td>SUPPORTED</td>\n","      <td>thegioi</td>\n","    </tr>\n","    <tr>\n","      <th>193742</th>\n","      <td>0</td>\n","      <td>19637</td>\n","      <td>phòng lao_động quận 7 ban quản_lý các khu chế_...</td>\n","      <td>khảo_sát của công_đoàn cho thấy khoảng 240 lao...</td>\n","      <td>hơn tháng qua công_đoàn đã trực_tiếp liên_hệ v...</td>\n","      <td>SUPPORTED</td>\n","      <td>thoisu</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>11261</th>\n","      <td>0</td>\n","      <td>7631</td>\n","      <td>tháng 4/2020 là thời_kỳ đỉnh_điểm nhất của cov...</td>\n","      <td>đặc_biệt sản_phẩm này có tiềm_năng hỗ_trợ ngườ...</td>\n","      <td>giai_đoạn cao_điểm covid19 hồi tháng 4 năm 202...</td>\n","      <td>SUPPORTED</td>\n","      <td>khoahoc</td>\n","    </tr>\n","    <tr>\n","      <th>48056</th>\n","      <td>0</td>\n","      <td>9398</td>\n","      <td>vật_liệu điện_sắc được giới khoa_học quan_tâm ...</td>\n","      <td>thiết_bị điện_sắc được nhóm nghiên_cứu thử_ngh...</td>\n","      <td>theo nhóm vật_liệu điện_sắc được giới khoa_học...</td>\n","      <td>SUPPORTED</td>\n","      <td>khoahoc</td>\n","    </tr>\n","    <tr>\n","      <th>77049</th>\n","      <td>0</td>\n","      <td>10830</td>\n","      <td>những chiến_thắng được ca_ngợi giúp prigozhin ...</td>\n","      <td>các thành_viên wagner được cho là đã âm_thầm t...</td>\n","      <td>điều đó cũng giúp prigozhin tăng_cường ảnh_hưở...</td>\n","      <td>SUPPORTED</td>\n","      <td>thegioi</td>\n","    </tr>\n","    <tr>\n","      <th>198296</th>\n","      <td>0</td>\n","      <td>19911</td>\n","      <td>những bức ảnh được nhiếp_ảnh gia_john thomson ...</td>\n","      <td>áo_dài năm thân_may theo phong_cách kín_đáo đư...</td>\n","      <td>thiếu_nữ sài_gòn là một trong những bức ảnh đư...</td>\n","      <td>SUPPORTED</td>\n","      <td>thegioi</td>\n","    </tr>\n","    <tr>\n","      <th>71530</th>\n","      <td>0</td>\n","      <td>10592</td>\n","      <td>hợp_chất anthocyanin được trích_ly bằng cồn 70...</td>\n","      <td>tuy_nhiên nhóm là một trong những người tiên_p...</td>\n","      <td>hoa đậu_biếc được thu_thập sau đó trích_ly bằn...</td>\n","      <td>SUPPORTED</td>\n","      <td>khoahoc</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>336661 rows × 7 columns</p>\n","</div>"],"text/plain":["        evidence_label     id  \\\n","234309               0  30927   \n","223745               0  30428   \n","94193                0  11558   \n","114060               0  12370   \n","193742               0  19637   \n","...                ...    ...   \n","11261                0   7631   \n","48056                0   9398   \n","77049                0  10830   \n","198296               0  19911   \n","71530                0  10592   \n","\n","                                                    claim  \\\n","234309  khi gia_nhập college compass nhung được kết_nố...   \n","223745  từ lúc nghỉ_việc gia_đình chị thúy rất khó khắ...   \n","94193   cần nhiều công_nghệ và thời_gian để khắc_phục ...   \n","114060  ông trump không_thể tự ân_xá cho mình dù trở_t...   \n","193742  phòng lao_động quận 7 ban quản_lý các khu chế_...   \n","...                                                   ...   \n","11261   tháng 4/2020 là thời_kỳ đỉnh_điểm nhất của cov...   \n","48056   vật_liệu điện_sắc được giới khoa_học quan_tâm ...   \n","77049   những chiến_thắng được ca_ngợi giúp prigozhin ...   \n","198296  những bức ảnh được nhiếp_ảnh gia_john thomson ...   \n","71530   hợp_chất anthocyanin được trích_ly bằng cồn 70...   \n","\n","                                                 sentence  \\\n","234309  trong bài_luận gửi đến trường harvard nữ_sinh_...   \n","223745                                           lê tuyết   \n","94193   khi tìm_kiếm sản_phẩm chăm_sóc da đặc_biệt cho...   \n","114060  quá_trình xử_lý vụ án có_thể diễn ra trong thờ...   \n","193742  khảo_sát của công_đoàn cho thấy khoảng 240 lao...   \n","...                                                   ...   \n","11261   đặc_biệt sản_phẩm này có tiềm_năng hỗ_trợ ngườ...   \n","48056   thiết_bị điện_sắc được nhóm nghiên_cứu thử_ngh...   \n","77049   các thành_viên wagner được cho là đã âm_thầm t...   \n","198296  áo_dài năm thân_may theo phong_cách kín_đáo đư...   \n","71530   tuy_nhiên nhóm là một trong những người tiên_p...   \n","\n","                                                 evidence    verdict   domain  \n","234309  khi trở_thành học_sinh của college compass nhu...  SUPPORTED  giaoduc  \n","223745  từ lúc nghỉ_việc gia_đình chị thúy chỉ kiếm đư...  SUPPORTED   thoisu  \n","94193   bác_sĩ thành cho rằng trường_hợp bệnh_nhân trê...  SUPPORTED  suckhoe  \n","114060  nếu trở_thành tổng_thống ông trump không nắm g...  SUPPORTED  thegioi  \n","193742  hơn tháng qua công_đoàn đã trực_tiếp liên_hệ v...  SUPPORTED   thoisu  \n","...                                                   ...        ...      ...  \n","11261   giai_đoạn cao_điểm covid19 hồi tháng 4 năm 202...  SUPPORTED  khoahoc  \n","48056   theo nhóm vật_liệu điện_sắc được giới khoa_học...  SUPPORTED  khoahoc  \n","77049   điều đó cũng giúp prigozhin tăng_cường ảnh_hưở...  SUPPORTED  thegioi  \n","198296  thiếu_nữ sài_gòn là một trong những bức ảnh đư...  SUPPORTED  thegioi  \n","71530   hoa đậu_biếc được thu_thập sau đó trích_ly bằn...  SUPPORTED  khoahoc  \n","\n","[336661 rows x 7 columns]"]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["df"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7,"status":"ok","timestamp":1698906096686,"user":{"displayName":"Anh Đức Bùi","userId":"02440975349884324274"},"user_tz":-420},"id":"Glgh11a1xgBg","outputId":"666fc26b-aeb5-4868-b382-9fb76c2f4975"},"outputs":[{"data":{"text/plain":["evidence_label\n","0    322111\n","1     14550\n","Name: count, dtype: int64"]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["df[\"evidence_label\"].value_counts()"]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":424},"executionInfo":{"elapsed":1035,"status":"ok","timestamp":1698906097717,"user":{"displayName":"Anh Đức Bùi","userId":"02440975349884324274"},"user_tz":-420},"id":"fzmpHFjDyEis","outputId":"64fee93f-efb7-434b-d463-34a5f82e936f"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>evidence_label</th>\n","      <th>id</th>\n","      <th>claim</th>\n","      <th>sentence</th>\n","      <th>evidence</th>\n","      <th>verdict</th>\n","      <th>domain</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>223745</th>\n","      <td>0</td>\n","      <td>30428</td>\n","      <td>từ lúc nghỉ_việc gia_đình chị thúy rất khó khắ...</td>\n","      <td>lê tuyết</td>\n","      <td>từ lúc nghỉ_việc gia_đình chị thúy chỉ kiếm đư...</td>\n","      <td>SUPPORTED</td>\n","      <td>thoisu</td>\n","    </tr>\n","    <tr>\n","      <th>54650</th>\n","      <td>0</td>\n","      <td>9735</td>\n","      <td>giảo cổ_lam</td>\n","      <td>thanh hoa</td>\n","      <td>giảo cổ_lam trong giảo cổ_lam có chứa phanosid</td>\n","      <td>SUPPORTED</td>\n","      <td>suckhoe</td>\n","    </tr>\n","    <tr>\n","      <th>162702</th>\n","      <td>0</td>\n","      <td>14185</td>\n","      <td>đầu năm 2022 dự_án sẽ đi vào hoạt_động</td>\n","      <td>để phòng nguy_hiểm</td>\n","      <td>đầu năm 2022 công_trình đi vào hoạt_động</td>\n","      <td>SUPPORTED</td>\n","      <td>thoisu</td>\n","    </tr>\n","    <tr>\n","      <th>160332</th>\n","      <td>0</td>\n","      <td>14075</td>\n","      <td>rạng sáng nay người_dân khu_vực gần cầu kênh n...</td>\n","      <td>ngọc tài nguyễn_khánh</td>\n","      <td>rạng sáng nay người_dân gần cầu bắc qua kênh n...</td>\n","      <td>SUPPORTED</td>\n","      <td>thoisu</td>\n","    </tr>\n","    <tr>\n","      <th>214061</th>\n","      <td>0</td>\n","      <td>30071</td>\n","      <td>trong năm đầu_tiên cô đã hoàn_thành_phần nghiê...</td>\n","      <td>thanh hằng</td>\n","      <td>năm đầu_tiên cô hoàn_thành_phần nghiên_cứu lý_...</td>\n","      <td>SUPPORTED</td>\n","      <td>giaoduc</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>227131</th>\n","      <td>0</td>\n","      <td>30588</td>\n","      <td>trong thông_báo vừa gửi 7 hãng taxi về giá dịc...</td>\n","      <td>gia_minh</td>\n","      <td>trong thông_báo vừa gửi 7 hãng taxi về giá dịc...</td>\n","      <td>SUPPORTED</td>\n","      <td>thoisu</td>\n","    </tr>\n","    <tr>\n","      <th>198769</th>\n","      <td>0</td>\n","      <td>19936</td>\n","      <td>yoga xuất_hiện từ thời ấn_độ cổ_đại</td>\n","      <td>bảo đức</td>\n","      <td>yoga là bộ_môn truyền_thống năm nghìn năm_tuổi...</td>\n","      <td>SUPPORTED</td>\n","      <td>thegioi</td>\n","    </tr>\n","    <tr>\n","      <th>185884</th>\n","      <td>1</td>\n","      <td>19243</td>\n","      <td>dự_kiến tổng mức đầu_tư cho dự_án cảng_hàng_kh...</td>\n","      <td>phước tuấn_gia chính</td>\n","      <td>phước tuấn_gia chính</td>\n","      <td>SUPPORTED</td>\n","      <td>thoisu</td>\n","    </tr>\n","    <tr>\n","      <th>306027</th>\n","      <td>0</td>\n","      <td>46364</td>\n","      <td>ông nguyễn_quang_thái thành_viên hội_đồng tư_v...</td>\n","      <td>hà_an</td>\n","      <td>ông nguyễn_quang_thái thành_viên hội_đồng tư_v...</td>\n","      <td>REFUTED</td>\n","      <td>khoahoc</td>\n","    </tr>\n","    <tr>\n","      <th>227018</th>\n","      <td>0</td>\n","      <td>30581</td>\n","      <td>nhiều xe đi trái quy_định</td>\n","      <td>việt quốc</td>\n","      <td>một_số ôtô quá_tải phóng vào khi đang thảm_nhự...</td>\n","      <td>SUPPORTED</td>\n","      <td>thoisu</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>7345 rows × 7 columns</p>\n","</div>"],"text/plain":["        evidence_label     id  \\\n","223745               0  30428   \n","54650                0   9735   \n","162702               0  14185   \n","160332               0  14075   \n","214061               0  30071   \n","...                ...    ...   \n","227131               0  30588   \n","198769               0  19936   \n","185884               1  19243   \n","306027               0  46364   \n","227018               0  30581   \n","\n","                                                    claim  \\\n","223745  từ lúc nghỉ_việc gia_đình chị thúy rất khó khắ...   \n","54650                                         giảo cổ_lam   \n","162702             đầu năm 2022 dự_án sẽ đi vào hoạt_động   \n","160332  rạng sáng nay người_dân khu_vực gần cầu kênh n...   \n","214061  trong năm đầu_tiên cô đã hoàn_thành_phần nghiê...   \n","...                                                   ...   \n","227131  trong thông_báo vừa gửi 7 hãng taxi về giá dịc...   \n","198769                yoga xuất_hiện từ thời ấn_độ cổ_đại   \n","185884  dự_kiến tổng mức đầu_tư cho dự_án cảng_hàng_kh...   \n","306027  ông nguyễn_quang_thái thành_viên hội_đồng tư_v...   \n","227018                          nhiều xe đi trái quy_định   \n","\n","                     sentence  \\\n","223745               lê tuyết   \n","54650               thanh hoa   \n","162702     để phòng nguy_hiểm   \n","160332  ngọc tài nguyễn_khánh   \n","214061             thanh hằng   \n","...                       ...   \n","227131               gia_minh   \n","198769                bảo đức   \n","185884   phước tuấn_gia chính   \n","306027                  hà_an   \n","227018              việt quốc   \n","\n","                                                 evidence    verdict   domain  \n","223745  từ lúc nghỉ_việc gia_đình chị thúy chỉ kiếm đư...  SUPPORTED   thoisu  \n","54650      giảo cổ_lam trong giảo cổ_lam có chứa phanosid  SUPPORTED  suckhoe  \n","162702           đầu năm 2022 công_trình đi vào hoạt_động  SUPPORTED   thoisu  \n","160332  rạng sáng nay người_dân gần cầu bắc qua kênh n...  SUPPORTED   thoisu  \n","214061  năm đầu_tiên cô hoàn_thành_phần nghiên_cứu lý_...  SUPPORTED  giaoduc  \n","...                                                   ...        ...      ...  \n","227131  trong thông_báo vừa gửi 7 hãng taxi về giá dịc...  SUPPORTED   thoisu  \n","198769  yoga là bộ_môn truyền_thống năm nghìn năm_tuổi...  SUPPORTED  thegioi  \n","185884                               phước tuấn_gia chính  SUPPORTED   thoisu  \n","306027  ông nguyễn_quang_thái thành_viên hội_đồng tư_v...    REFUTED  khoahoc  \n","227018  một_số ôtô quá_tải phóng vào khi đang thảm_nhự...  SUPPORTED   thoisu  \n","\n","[7345 rows x 7 columns]"]},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":["filtered_df = df.loc[(df['sentence'].apply(lambda x: len(x.split())) < 4)]\n","filtered_df"]},{"cell_type":"code","execution_count":11,"metadata":{"executionInfo":{"elapsed":496,"status":"ok","timestamp":1698906098210,"user":{"displayName":"Anh Đức Bùi","userId":"02440975349884324274"},"user_tz":-420},"id":"Xd5yf-nOyr58"},"outputs":[],"source":["df = df.drop(df[df.isin(filtered_df.to_dict('list')).all(1)].index)\n"]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":597},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1698906098210,"user":{"displayName":"Anh Đức Bùi","userId":"02440975349884324274"},"user_tz":-420},"id":"0DeUl-Hhyxta","outputId":"01d2a96b-00e9-47a6-89dd-480512f8d537"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>evidence_label</th>\n","      <th>id</th>\n","      <th>claim</th>\n","      <th>sentence</th>\n","      <th>evidence</th>\n","      <th>verdict</th>\n","      <th>domain</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>234309</th>\n","      <td>0</td>\n","      <td>30927</td>\n","      <td>khi gia_nhập college compass nhung được kết_nố...</td>\n","      <td>trong bài_luận gửi đến trường harvard nữ_sinh_...</td>\n","      <td>khi trở_thành học_sinh của college compass nhu...</td>\n","      <td>SUPPORTED</td>\n","      <td>giaoduc</td>\n","    </tr>\n","    <tr>\n","      <th>94193</th>\n","      <td>0</td>\n","      <td>11558</td>\n","      <td>cần nhiều công_nghệ và thời_gian để khắc_phục ...</td>\n","      <td>khi tìm_kiếm sản_phẩm chăm_sóc da đặc_biệt cho...</td>\n","      <td>bác_sĩ thành cho rằng trường_hợp bệnh_nhân trê...</td>\n","      <td>SUPPORTED</td>\n","      <td>suckhoe</td>\n","    </tr>\n","    <tr>\n","      <th>114060</th>\n","      <td>0</td>\n","      <td>12370</td>\n","      <td>ông trump không_thể tự ân_xá cho mình dù trở_t...</td>\n","      <td>quá_trình xử_lý vụ án có_thể diễn ra trong thờ...</td>\n","      <td>nếu trở_thành tổng_thống ông trump không nắm g...</td>\n","      <td>SUPPORTED</td>\n","      <td>thegioi</td>\n","    </tr>\n","    <tr>\n","      <th>193742</th>\n","      <td>0</td>\n","      <td>19637</td>\n","      <td>phòng lao_động quận 7 ban quản_lý các khu chế_...</td>\n","      <td>khảo_sát của công_đoàn cho thấy khoảng 240 lao...</td>\n","      <td>hơn tháng qua công_đoàn đã trực_tiếp liên_hệ v...</td>\n","      <td>SUPPORTED</td>\n","      <td>thoisu</td>\n","    </tr>\n","    <tr>\n","      <th>349</th>\n","      <td>0</td>\n","      <td>7145</td>\n","      <td>số_liệu của viện phát_triển giáo_dục hàn_quốc ...</td>\n","      <td>tôi nghĩ không nhiều đứa trẻ gây ra nỗi đau_th...</td>\n","      <td>lee cũng cho rằng nhiều học_sinh đã mất niềm t...</td>\n","      <td>SUPPORTED</td>\n","      <td>giaoduc</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>11261</th>\n","      <td>0</td>\n","      <td>7631</td>\n","      <td>tháng 4/2020 là thời_kỳ đỉnh_điểm nhất của cov...</td>\n","      <td>đặc_biệt sản_phẩm này có tiềm_năng hỗ_trợ ngườ...</td>\n","      <td>giai_đoạn cao_điểm covid19 hồi tháng 4 năm 202...</td>\n","      <td>SUPPORTED</td>\n","      <td>khoahoc</td>\n","    </tr>\n","    <tr>\n","      <th>48056</th>\n","      <td>0</td>\n","      <td>9398</td>\n","      <td>vật_liệu điện_sắc được giới khoa_học quan_tâm ...</td>\n","      <td>thiết_bị điện_sắc được nhóm nghiên_cứu thử_ngh...</td>\n","      <td>theo nhóm vật_liệu điện_sắc được giới khoa_học...</td>\n","      <td>SUPPORTED</td>\n","      <td>khoahoc</td>\n","    </tr>\n","    <tr>\n","      <th>77049</th>\n","      <td>0</td>\n","      <td>10830</td>\n","      <td>những chiến_thắng được ca_ngợi giúp prigozhin ...</td>\n","      <td>các thành_viên wagner được cho là đã âm_thầm t...</td>\n","      <td>điều đó cũng giúp prigozhin tăng_cường ảnh_hưở...</td>\n","      <td>SUPPORTED</td>\n","      <td>thegioi</td>\n","    </tr>\n","    <tr>\n","      <th>198296</th>\n","      <td>0</td>\n","      <td>19911</td>\n","      <td>những bức ảnh được nhiếp_ảnh gia_john thomson ...</td>\n","      <td>áo_dài năm thân_may theo phong_cách kín_đáo đư...</td>\n","      <td>thiếu_nữ sài_gòn là một trong những bức ảnh đư...</td>\n","      <td>SUPPORTED</td>\n","      <td>thegioi</td>\n","    </tr>\n","    <tr>\n","      <th>71530</th>\n","      <td>0</td>\n","      <td>10592</td>\n","      <td>hợp_chất anthocyanin được trích_ly bằng cồn 70...</td>\n","      <td>tuy_nhiên nhóm là một trong những người tiên_p...</td>\n","      <td>hoa đậu_biếc được thu_thập sau đó trích_ly bằn...</td>\n","      <td>SUPPORTED</td>\n","      <td>khoahoc</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>329316 rows × 7 columns</p>\n","</div>"],"text/plain":["        evidence_label     id  \\\n","234309               0  30927   \n","94193                0  11558   \n","114060               0  12370   \n","193742               0  19637   \n","349                  0   7145   \n","...                ...    ...   \n","11261                0   7631   \n","48056                0   9398   \n","77049                0  10830   \n","198296               0  19911   \n","71530                0  10592   \n","\n","                                                    claim  \\\n","234309  khi gia_nhập college compass nhung được kết_nố...   \n","94193   cần nhiều công_nghệ và thời_gian để khắc_phục ...   \n","114060  ông trump không_thể tự ân_xá cho mình dù trở_t...   \n","193742  phòng lao_động quận 7 ban quản_lý các khu chế_...   \n","349     số_liệu của viện phát_triển giáo_dục hàn_quốc ...   \n","...                                                   ...   \n","11261   tháng 4/2020 là thời_kỳ đỉnh_điểm nhất của cov...   \n","48056   vật_liệu điện_sắc được giới khoa_học quan_tâm ...   \n","77049   những chiến_thắng được ca_ngợi giúp prigozhin ...   \n","198296  những bức ảnh được nhiếp_ảnh gia_john thomson ...   \n","71530   hợp_chất anthocyanin được trích_ly bằng cồn 70...   \n","\n","                                                 sentence  \\\n","234309  trong bài_luận gửi đến trường harvard nữ_sinh_...   \n","94193   khi tìm_kiếm sản_phẩm chăm_sóc da đặc_biệt cho...   \n","114060  quá_trình xử_lý vụ án có_thể diễn ra trong thờ...   \n","193742  khảo_sát của công_đoàn cho thấy khoảng 240 lao...   \n","349     tôi nghĩ không nhiều đứa trẻ gây ra nỗi đau_th...   \n","...                                                   ...   \n","11261   đặc_biệt sản_phẩm này có tiềm_năng hỗ_trợ ngườ...   \n","48056   thiết_bị điện_sắc được nhóm nghiên_cứu thử_ngh...   \n","77049   các thành_viên wagner được cho là đã âm_thầm t...   \n","198296  áo_dài năm thân_may theo phong_cách kín_đáo đư...   \n","71530   tuy_nhiên nhóm là một trong những người tiên_p...   \n","\n","                                                 evidence    verdict   domain  \n","234309  khi trở_thành học_sinh của college compass nhu...  SUPPORTED  giaoduc  \n","94193   bác_sĩ thành cho rằng trường_hợp bệnh_nhân trê...  SUPPORTED  suckhoe  \n","114060  nếu trở_thành tổng_thống ông trump không nắm g...  SUPPORTED  thegioi  \n","193742  hơn tháng qua công_đoàn đã trực_tiếp liên_hệ v...  SUPPORTED   thoisu  \n","349     lee cũng cho rằng nhiều học_sinh đã mất niềm t...  SUPPORTED  giaoduc  \n","...                                                   ...        ...      ...  \n","11261   giai_đoạn cao_điểm covid19 hồi tháng 4 năm 202...  SUPPORTED  khoahoc  \n","48056   theo nhóm vật_liệu điện_sắc được giới khoa_học...  SUPPORTED  khoahoc  \n","77049   điều đó cũng giúp prigozhin tăng_cường ảnh_hưở...  SUPPORTED  thegioi  \n","198296  thiếu_nữ sài_gòn là một trong những bức ảnh đư...  SUPPORTED  thegioi  \n","71530   hoa đậu_biếc được thu_thập sau đó trích_ly bằn...  SUPPORTED  khoahoc  \n","\n","[329316 rows x 7 columns]"]},"execution_count":12,"metadata":{},"output_type":"execute_result"}],"source":["df"]},{"cell_type":"code","execution_count":13,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1698906098210,"user":{"displayName":"Anh Đức Bùi","userId":"02440975349884324274"},"user_tz":-420},"id":"Up4kB2gGElO2"},"outputs":[],"source":["df_eval_1 = df[df['evidence_label'] == 1].head(690)\n","df_eval_0 = df[df['evidence_label'] == 0].head(690)\n","df_eval = pd.concat([df_eval_0, df_eval_1])"]},{"cell_type":"code","execution_count":14,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1698906098210,"user":{"displayName":"Anh Đức Bùi","userId":"02440975349884324274"},"user_tz":-420},"id":"bZAZgNPjEkkD"},"outputs":[],"source":["df_train = df[~df.isin(df_eval).all(axis=1)]"]},{"cell_type":"code","execution_count":15,"metadata":{"executionInfo":{"elapsed":314,"status":"ok","timestamp":1698906098520,"user":{"displayName":"Anh Đức Bùi","userId":"02440975349884324274"},"user_tz":-420},"id":"nwcJZB1aFHtL"},"outputs":[],"source":["df_train_1 = df_train[df_train['evidence_label'] == 1]\n","df_train_0 = df_train[df_train['evidence_label'] == 0].head(len(df_train_1)*args.scale_pos_neg)\n","df_train = pd.concat([df_train_0, df_train_1])"]},{"cell_type":"code","execution_count":16,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10,"status":"ok","timestamp":1698906098521,"user":{"displayName":"Anh Đức Bùi","userId":"02440975349884324274"},"user_tz":-420},"id":"U5DIg7jc0sQ9","outputId":"39ca5f52-2747-43c3-bad3-bf3ba674a81d"},"outputs":[{"data":{"text/plain":["evidence_label\n","0    66280\n","1    13256\n","Name: count, dtype: int64"]},"execution_count":16,"metadata":{},"output_type":"execute_result"}],"source":["df_train['evidence_label'].value_counts()"]},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[{"data":{"text/plain":["0"]},"execution_count":17,"metadata":{},"output_type":"execute_result"}],"source":["df_train.duplicated().sum()"]},{"cell_type":"code","execution_count":18,"metadata":{"executionInfo":{"elapsed":9,"status":"ok","timestamp":1698906098521,"user":{"displayName":"Anh Đức Bùi","userId":"02440975349884324274"},"user_tz":-420},"id":"oU6wWL4gGxZc"},"outputs":[],"source":["# debug\n","df_eval = df_eval.head(10)\n","df_train = df_train.head(100)"]},{"cell_type":"markdown","metadata":{"id":"o9GnZNTMxVAq"},"source":["# Model"]},{"cell_type":"markdown","metadata":{"id":"NhiD5YPyxXvE"},"source":["## Colate function"]},{"cell_type":"code","execution_count":19,"metadata":{"executionInfo":{"elapsed":9,"status":"ok","timestamp":1698906098521,"user":{"displayName":"Anh Đức Bùi","userId":"02440975349884324274"},"user_tz":-420},"id":"s3noqJ_txTb2"},"outputs":[],"source":["def collate_to_max_length(batch: List[List[torch.Tensor]], max_len: int = None, fill_values: List[float] = None) -> \\\n","    List[torch.Tensor]:\n","    \"\"\"\n","    pad to maximum length of this batch\n","    Args:\n","        batch: a batch of samples, each contains a list of field data(Tensor), which shape is [seq_length]\n","        max_len: specify max length\n","        fill_values: specify filled values of each field\n","    Returns:\n","        output: list of field batched data, which shape is [batch, max_length]\n","    \"\"\"\n","    # [batch, num_fields]\n","    lengths = np.array([[len(field_data) for field_data in sample] for sample in batch])\n","    batch_size, num_fields = lengths.shape\n","    fill_values = fill_values or [0.0] * num_fields\n","    # [num_fields]\n","    max_lengths = lengths.max(axis=0)\n","    if max_len:\n","        assert max_lengths.max() <= max_len\n","        max_lengths = np.ones_like(max_lengths) * max_len\n","\n","    output = [torch.full([batch_size, max_lengths[field_idx]],\n","                         fill_value=fill_values[field_idx],\n","                         dtype=batch[0][field_idx].dtype)\n","              for field_idx in range(num_fields)]\n","    for sample_idx in range(batch_size):\n","        for field_idx in range(num_fields):\n","            # seq_length\n","            data = batch[sample_idx][field_idx]\n","            output[field_idx][sample_idx][: data.shape[0]] = data\n","    # generate span_index and span_mask\n","    max_sentence_length = max_lengths[0]\n","    start_indexs = []\n","    end_indexs = []\n","    for i in range(1, max_sentence_length - 1):\n","        for j in range(i, max_sentence_length - 1):\n","            # # span大小为10\n","            # if j - i > 10:\n","            #     continue\n","            start_indexs.append(i)\n","            end_indexs.append(j)\n","    # generate span mask\n","    span_masks = []\n","    for input_ids, label, length in batch:\n","        span_mask = []\n","        middle_index = input_ids.tolist().index(2)\n","        for start_index, end_index in zip(start_indexs, end_indexs):\n","            if 1 <= start_index <= length.item() - 2 and 1 <= end_index <= length.item() - 2 and (\n","                start_index > middle_index or end_index < middle_index):\n","                span_mask.append(0)\n","            else:\n","                span_mask.append(1e6)\n","        span_masks.append(span_mask)\n","    # add to output\n","    output.append(torch.LongTensor(start_indexs))\n","    output.append(torch.LongTensor(end_indexs))\n","    output.append(torch.LongTensor(span_masks))\n","    return output  # (input_ids, labels, length, start_indexs, end_indexs, span_masks)\n","\n"]},{"cell_type":"markdown","metadata":{"id":"GelyFb6DxgOy"},"source":["## Data loader"]},{"cell_type":"code","execution_count":20,"metadata":{"executionInfo":{"elapsed":9,"status":"ok","timestamp":1698906098521,"user":{"displayName":"Anh Đức Bùi","userId":"02440975349884324274"},"user_tz":-420},"id":"-4Te2nZHxkje"},"outputs":[],"source":["import json\n","import os\n","from functools import partial\n","\n","import torch\n","from torch.utils.data import Dataset, DataLoader\n","from transformers import RobertaTokenizer\n","\n","class DSC_Dataset(Dataset):\n","    def __init__(self, dataframe, bert_path, max_length: int = 512):\n","        super().__init__()\n","        self.max_length = max_length\n","        # label_map = {\"NON\": 0, 'SR': 1}\n","        # with open(os.path.join(directory, prefix + '.json')) as file:\n","        #     data = json.load(file)\n","        # self.result = data\n","        self.result = dataframe\n","        self.tokenizer = AutoTokenizer.from_pretrained(bert_path, use_fast=False)\n","        # self.tokenizer = RobertaTokenizer.from_pretrained(bert_path)\n","\n","    def __len__(self):\n","        return len(self.result)\n","\n","    def __getitem__(self, idx):\n","        # sentence_1, sentence_2, label = self.result[idx]\n","        # check for required keys\n","        # if not all(key in self.result[idx] for key in [\"claim\", \"sentence\", \"verdict\"]):\n","        #     raise KeyError(\"Missing key in result dictionary.\")\n","\n","        sentence_1 = self.result.iloc[idx][\"claim\"]\n","        sentence_2 = self.result.iloc[idx][\"sentence\"]\n","        label = self.result.iloc[idx][\"evidence_label\"]\n","        # remove .\n","        if sentence_1.endswith(\".\"):\n","            sentence_1 = sentence_1[:-1]\n","        if sentence_2.endswith(\".\"):\n","            sentence_2 = sentence_2[:-1]\n","        sentence_1_input_ids = self.tokenizer.encode(\n","            sentence_1, add_special_tokens=False)\n","        sentence_2_input_ids = self.tokenizer.encode(\n","            sentence_2, add_special_tokens=False)\n","        input_ids = sentence_1_input_ids + [2] + sentence_2_input_ids\n","        if len(input_ids) > self.max_length - 2:\n","            input_ids = input_ids[:self.max_length - 2]\n","        # convert list to tensor\n","        length = torch.LongTensor([len(input_ids) + 2])\n","        input_ids = torch.LongTensor([0] + input_ids + [2])\n","        label = torch.LongTensor([label])\n","        return input_ids, label, length\n","\n","\n","# def unit_test():\n","#     root_path = \"D:\\\\DUCBUI\\\\DSC\\\\self_explanining\\\\data\\\\\"\n","#     bert_path = \"roberta-base\"\n","#     prefix = \"train_evidence\"\n","#     dataset = DSC_Dataset(directory=root_path,\n","#                           prefix=prefix, bert_path=bert_path)\n","\n","#     print(dataset)\n","\n","\n","\n","# if __name__ == '__main__':\n","#     unit_test()"]},{"cell_type":"code","execution_count":21,"metadata":{"executionInfo":{"elapsed":8,"status":"ok","timestamp":1698906098521,"user":{"displayName":"Anh Đức Bùi","userId":"02440975349884324274"},"user_tz":-420},"id":"ZFmItc2QxqSM"},"outputs":[],"source":["class ExplainableModel(nn.Module):\n","    def __init__(self, bert_dir):\n","        super().__init__()\n","        self.bert_config = RobertaConfig.from_pretrained(\n","            bert_dir, output_hidden_states=True)\n","        self.intermediate = RobertaModel.from_pretrained(bert_dir)\n","        self.span_info_collect = SICModel(self.bert_config.hidden_size)\n","        self.interpretation = InterpretationModel(self.bert_config.hidden_size)\n","        self.output = nn.Linear(\n","            self.bert_config.hidden_size, self.bert_config.num_labels)\n","\n","    def forward(self, input_ids, start_indexs, end_indexs, span_masks):\n","        # generate mask\n","        attention_mask = (input_ids != 1).long()\n","        # intermediate layer\n","        a = self.intermediate(\n","            input_ids, attention_mask=attention_mask)\n","        hidden_states = self.intermediate(\n","            input_ids, attention_mask=attention_mask).last_hidden_state\n","        # span info collecting layer(SIC)\n","        h_ij = self.span_info_collect(hidden_states, start_indexs, end_indexs)\n","        # interpretation layer\n","        H, a_ij = self.interpretation(h_ij, span_masks)\n","        # output layer\n","        out = self.output(H)\n","        return out, a_ij\n","\n","\n","class SICModel(nn.Module):\n","    def __init__(self, hidden_size):\n","        super().__init__()\n","        self.hidden_size = hidden_size\n","\n","        self.W_1 = nn.Linear(hidden_size, hidden_size)\n","        self.W_2 = nn.Linear(hidden_size, hidden_size)\n","        self.W_3 = nn.Linear(hidden_size, hidden_size)\n","        self.W_4 = nn.Linear(hidden_size, hidden_size)\n","\n","    def forward(self, hidden_states, start_indexs, end_indexs):\n","        W1_h = self.W_1(hidden_states)  # (bs, length, hidden_size)\n","        W2_h = self.W_2(hidden_states)\n","        W3_h = self.W_3(hidden_states)\n","        W4_h = self.W_4(hidden_states)\n","\n","        # (bs, span_num, hidden_size)\n","        W1_hi_emb = torch.index_select(W1_h, 1, start_indexs)\n","        W2_hj_emb = torch.index_select(W2_h, 1, end_indexs)\n","        W3_hi_start_emb = torch.index_select(W3_h, 1, start_indexs)\n","        W3_hi_end_emb = torch.index_select(W3_h, 1, end_indexs)\n","        W4_hj_start_emb = torch.index_select(W4_h, 1, start_indexs)\n","        W4_hj_end_emb = torch.index_select(W4_h, 1, end_indexs)\n","\n","        # [w1*hi, w2*hj, w3(hi-hj), w4(hi⊗hj)]\n","        span = W1_hi_emb + W2_hj_emb + \\\n","            (W3_hi_start_emb - W3_hi_end_emb) + \\\n","            torch.mul(W4_hj_start_emb, W4_hj_end_emb)\n","        h_ij = torch.tanh(span)\n","        return h_ij\n","\n","\n","class InterpretationModel(nn.Module):\n","    def __init__(self, hidden_size):\n","        super().__init__()\n","        self.h_t = nn.Linear(hidden_size, 1)\n","\n","    def forward(self, h_ij, span_masks):\n","        o_ij = self.h_t(h_ij).squeeze(-1)  # (ba, span_num)\n","        # mask illegal span\n","        o_ij = o_ij - span_masks\n","        # normalize all a_ij, a_ij sum = 1\n","        a_ij = nn.functional.softmax(o_ij, dim=1)\n","        # weight average span representation to get H\n","        H = (a_ij.unsqueeze(-1) * h_ij).sum(dim=1)  # (bs, hidden_size)\n","        return H, a_ij\n","\n","\n","# def main():\n","#     # data\n","#     input_id_1 = torch.LongTensor([0, 4, 5, 6, 7, 2])\n","#     input_id_2 = torch.LongTensor([0, 4, 5, 2])\n","#     input_id_3 = torch.LongTensor([0, 4, 2])\n","#     batch = [(input_id_1, torch.LongTensor([1]), torch.LongTensor([6])),\n","#              (input_id_2, torch.LongTensor([1]), torch.LongTensor([4])),\n","#              (input_id_3, torch.LongTensor([1]), torch.LongTensor([3]))]\n","\n","#     output = collate_to_max_length(batch=batch, fill_values=[1, 0, 0])\n","#     input_ids, labels, length, start_indexs, end_indexs, span_masks = output\n","\n","#     # model\n","#     bert_path = \"roberta-base\"\n","#     model = ExplainableModel(bert_path)\n","#     print(model)\n","\n","#     output = model(input_ids, start_indexs, end_indexs, span_masks)\n","#     print(output[0])\n","\n","\n","# if __name__ == '__main__':\n","#     main()\n"]},{"cell_type":"code","execution_count":22,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":136,"referenced_widgets":["19d42d40dc414d96a60ef7fbfae234e8","6496d328ec194aa696eab15cecd8d2a1","0ad409e11bc2492b85fac1474cdfda0d","434fcd35068c4475a0854ed38740f44a","13cba4ab9e184cebbbc2e1dd3dc2259f","6bfd3395e8f342cdae384ddd697ffabb","ff8a5e1fd6ca4f70a6d3ac7656a7c419","e1635762ae2a490ca260195156d58d89","392e24892f584c578537d451c68f2925","965e38a7014c42e483e60b5d3c0771f0","53328ff985554e288124d0a5c9c9d24f","5ed20875c5d64a91822b6ef1118dca9d","2b7730212a7f4c26a79ed591dc805b04","c6fbe38396aa4b2aa301959deea7521f","d8a475e1c2814823add0a0b9a3e71bcc","25746fcd276c44b8a637aa7cc3b2ba27","704685f13e0d44e7ba1080e02599f2a7","c126d05c297841439f0b852867388ffb","935018870c144874ad93a7e0389b0d73","03498195cabc4afdb6e9216edcb5887a","41cd42366ae94ec3be4491277641d2b6","fe2162d8a06a49939125ad03e8dee2eb"]},"executionInfo":{"elapsed":9900,"status":"ok","timestamp":1698906108413,"user":{"displayName":"Anh Đức Bùi","userId":"02440975349884324274"},"user_tz":-420},"id":"-OlCZq283-mc","outputId":"e0d20d72-ceae-482a-969f-1fd369617d35"},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of RobertaModel were not initialized from the model checkpoint at vinai/phobert-base-v2 and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}],"source":["model = ExplainableModel(args.bert_path)"]},{"cell_type":"code","execution_count":23,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":32,"status":"ok","timestamp":1698906108413,"user":{"displayName":"Anh Đức Bùi","userId":"02440975349884324274"},"user_tz":-420},"id":"00-QNu8T4DZU","outputId":"f654781d-fc48-4c62-851c-afa0fe8d49cc"},"outputs":[{"data":{"text/plain":["ExplainableModel(\n","  (intermediate): RobertaModel(\n","    (embeddings): RobertaEmbeddings(\n","      (word_embeddings): Embedding(64001, 768, padding_idx=1)\n","      (position_embeddings): Embedding(258, 768, padding_idx=1)\n","      (token_type_embeddings): Embedding(1, 768)\n","      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (encoder): RobertaEncoder(\n","      (layer): ModuleList(\n","        (0-11): 12 x RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","    (pooler): RobertaPooler(\n","      (dense): Linear(in_features=768, out_features=768, bias=True)\n","      (activation): Tanh()\n","    )\n","  )\n","  (span_info_collect): SICModel(\n","    (W_1): Linear(in_features=768, out_features=768, bias=True)\n","    (W_2): Linear(in_features=768, out_features=768, bias=True)\n","    (W_3): Linear(in_features=768, out_features=768, bias=True)\n","    (W_4): Linear(in_features=768, out_features=768, bias=True)\n","  )\n","  (interpretation): InterpretationModel(\n","    (h_t): Linear(in_features=768, out_features=1, bias=True)\n","  )\n","  (output): Linear(in_features=768, out_features=2, bias=True)\n",")"]},"execution_count":23,"metadata":{},"output_type":"execute_result"}],"source":["model"]},{"cell_type":"code","execution_count":24,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":31,"status":"ok","timestamp":1698906108413,"user":{"displayName":"Anh Đức Bùi","userId":"02440975349884324274"},"user_tz":-420},"id":"s2lo4wlV4HV3","outputId":"1f603ccd-2f96-4ee3-e491-bace6859819a"},"outputs":[{"name":"stdout","output_type":"stream","text":["*******\n","ExplainableModel(\n","  (intermediate): RobertaModel(\n","    (embeddings): RobertaEmbeddings(\n","      (word_embeddings): Embedding(64001, 768, padding_idx=1)\n","      (position_embeddings): Embedding(258, 768, padding_idx=1)\n","      (token_type_embeddings): Embedding(1, 768)\n","      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (encoder): RobertaEncoder(\n","      (layer): ModuleList(\n","        (0-11): 12 x RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","    (pooler): RobertaPooler(\n","      (dense): Linear(in_features=768, out_features=768, bias=True)\n","      (activation): Tanh()\n","    )\n","  )\n","  (span_info_collect): SICModel(\n","    (W_1): Linear(in_features=768, out_features=768, bias=True)\n","    (W_2): Linear(in_features=768, out_features=768, bias=True)\n","    (W_3): Linear(in_features=768, out_features=768, bias=True)\n","    (W_4): Linear(in_features=768, out_features=768, bias=True)\n","  )\n","  (interpretation): InterpretationModel(\n","    (h_t): Linear(in_features=768, out_features=1, bias=True)\n","  )\n","  (output): Linear(in_features=768, out_features=2, bias=True)\n",")\n","intermediate*******\n","RobertaModel(\n","  (embeddings): RobertaEmbeddings(\n","    (word_embeddings): Embedding(64001, 768, padding_idx=1)\n","    (position_embeddings): Embedding(258, 768, padding_idx=1)\n","    (token_type_embeddings): Embedding(1, 768)\n","    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","    (dropout): Dropout(p=0.1, inplace=False)\n","  )\n","  (encoder): RobertaEncoder(\n","    (layer): ModuleList(\n","      (0-11): 12 x RobertaLayer(\n","        (attention): RobertaAttention(\n","          (self): RobertaSelfAttention(\n","            (query): Linear(in_features=768, out_features=768, bias=True)\n","            (key): Linear(in_features=768, out_features=768, bias=True)\n","            (value): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): RobertaSelfOutput(\n","            (dense): Linear(in_features=768, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): RobertaIntermediate(\n","          (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          (intermediate_act_fn): GELUActivation()\n","        )\n","        (output): RobertaOutput(\n","          (dense): Linear(in_features=3072, out_features=768, bias=True)\n","          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","    )\n","  )\n","  (pooler): RobertaPooler(\n","    (dense): Linear(in_features=768, out_features=768, bias=True)\n","    (activation): Tanh()\n","  )\n",")\n","intermediate.embeddings*******\n","RobertaEmbeddings(\n","  (word_embeddings): Embedding(64001, 768, padding_idx=1)\n","  (position_embeddings): Embedding(258, 768, padding_idx=1)\n","  (token_type_embeddings): Embedding(1, 768)\n","  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","  (dropout): Dropout(p=0.1, inplace=False)\n",")\n","intermediate.embeddings.word_embeddings*******\n","Embedding(64001, 768, padding_idx=1)\n","intermediate.embeddings.position_embeddings*******\n","Embedding(258, 768, padding_idx=1)\n","intermediate.embeddings.token_type_embeddings*******\n","Embedding(1, 768)\n","intermediate.embeddings.LayerNorm*******\n","LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","intermediate.embeddings.dropout*******\n","Dropout(p=0.1, inplace=False)\n","intermediate.encoder*******\n","RobertaEncoder(\n","  (layer): ModuleList(\n","    (0-11): 12 x RobertaLayer(\n","      (attention): RobertaAttention(\n","        (self): RobertaSelfAttention(\n","          (query): Linear(in_features=768, out_features=768, bias=True)\n","          (key): Linear(in_features=768, out_features=768, bias=True)\n","          (value): Linear(in_features=768, out_features=768, bias=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (output): RobertaSelfOutput(\n","          (dense): Linear(in_features=768, out_features=768, bias=True)\n","          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (intermediate): RobertaIntermediate(\n","        (dense): Linear(in_features=768, out_features=3072, bias=True)\n","        (intermediate_act_fn): GELUActivation()\n","      )\n","      (output): RobertaOutput(\n","        (dense): Linear(in_features=3072, out_features=768, bias=True)\n","        (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","    )\n","  )\n",")\n","intermediate.encoder.layer*******\n","ModuleList(\n","  (0-11): 12 x RobertaLayer(\n","    (attention): RobertaAttention(\n","      (self): RobertaSelfAttention(\n","        (query): Linear(in_features=768, out_features=768, bias=True)\n","        (key): Linear(in_features=768, out_features=768, bias=True)\n","        (value): Linear(in_features=768, out_features=768, bias=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (output): RobertaSelfOutput(\n","        (dense): Linear(in_features=768, out_features=768, bias=True)\n","        (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","    )\n","    (intermediate): RobertaIntermediate(\n","      (dense): Linear(in_features=768, out_features=3072, bias=True)\n","      (intermediate_act_fn): GELUActivation()\n","    )\n","    (output): RobertaOutput(\n","      (dense): Linear(in_features=3072, out_features=768, bias=True)\n","      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","  )\n",")\n","intermediate.encoder.layer.0*******\n","RobertaLayer(\n","  (attention): RobertaAttention(\n","    (self): RobertaSelfAttention(\n","      (query): Linear(in_features=768, out_features=768, bias=True)\n","      (key): Linear(in_features=768, out_features=768, bias=True)\n","      (value): Linear(in_features=768, out_features=768, bias=True)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (output): RobertaSelfOutput(\n","      (dense): Linear(in_features=768, out_features=768, bias=True)\n","      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","  )\n","  (intermediate): RobertaIntermediate(\n","    (dense): Linear(in_features=768, out_features=3072, bias=True)\n","    (intermediate_act_fn): GELUActivation()\n","  )\n","  (output): RobertaOutput(\n","    (dense): Linear(in_features=3072, out_features=768, bias=True)\n","    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","    (dropout): Dropout(p=0.1, inplace=False)\n","  )\n",")\n","intermediate.encoder.layer.0.attention*******\n","RobertaAttention(\n","  (self): RobertaSelfAttention(\n","    (query): Linear(in_features=768, out_features=768, bias=True)\n","    (key): Linear(in_features=768, out_features=768, bias=True)\n","    (value): Linear(in_features=768, out_features=768, bias=True)\n","    (dropout): Dropout(p=0.1, inplace=False)\n","  )\n","  (output): RobertaSelfOutput(\n","    (dense): Linear(in_features=768, out_features=768, bias=True)\n","    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","    (dropout): Dropout(p=0.1, inplace=False)\n","  )\n",")\n","intermediate.encoder.layer.0.attention.self*******\n","RobertaSelfAttention(\n","  (query): Linear(in_features=768, out_features=768, bias=True)\n","  (key): Linear(in_features=768, out_features=768, bias=True)\n","  (value): Linear(in_features=768, out_features=768, bias=True)\n","  (dropout): Dropout(p=0.1, inplace=False)\n",")\n","intermediate.encoder.layer.0.attention.self.query*******\n","Linear(in_features=768, out_features=768, bias=True)\n","intermediate.encoder.layer.0.attention.self.key*******\n","Linear(in_features=768, out_features=768, bias=True)\n","intermediate.encoder.layer.0.attention.self.value*******\n","Linear(in_features=768, out_features=768, bias=True)\n","intermediate.encoder.layer.0.attention.self.dropout*******\n","Dropout(p=0.1, inplace=False)\n","intermediate.encoder.layer.0.attention.output*******\n","RobertaSelfOutput(\n","  (dense): Linear(in_features=768, out_features=768, bias=True)\n","  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","  (dropout): Dropout(p=0.1, inplace=False)\n",")\n","intermediate.encoder.layer.0.attention.output.dense*******\n","Linear(in_features=768, out_features=768, bias=True)\n","intermediate.encoder.layer.0.attention.output.LayerNorm*******\n","LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","intermediate.encoder.layer.0.attention.output.dropout*******\n","Dropout(p=0.1, inplace=False)\n","intermediate.encoder.layer.0.intermediate*******\n","RobertaIntermediate(\n","  (dense): Linear(in_features=768, out_features=3072, bias=True)\n","  (intermediate_act_fn): GELUActivation()\n",")\n","intermediate.encoder.layer.0.intermediate.dense*******\n","Linear(in_features=768, out_features=3072, bias=True)\n","intermediate.encoder.layer.0.intermediate.intermediate_act_fn*******\n","GELUActivation()\n","intermediate.encoder.layer.0.output*******\n","RobertaOutput(\n","  (dense): Linear(in_features=3072, out_features=768, bias=True)\n","  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","  (dropout): Dropout(p=0.1, inplace=False)\n",")\n","intermediate.encoder.layer.0.output.dense*******\n","Linear(in_features=3072, out_features=768, bias=True)\n","intermediate.encoder.layer.0.output.LayerNorm*******\n","LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","intermediate.encoder.layer.0.output.dropout*******\n","Dropout(p=0.1, inplace=False)\n","intermediate.encoder.layer.1*******\n","RobertaLayer(\n","  (attention): RobertaAttention(\n","    (self): RobertaSelfAttention(\n","      (query): Linear(in_features=768, out_features=768, bias=True)\n","      (key): Linear(in_features=768, out_features=768, bias=True)\n","      (value): Linear(in_features=768, out_features=768, bias=True)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (output): RobertaSelfOutput(\n","      (dense): Linear(in_features=768, out_features=768, bias=True)\n","      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","  )\n","  (intermediate): RobertaIntermediate(\n","    (dense): Linear(in_features=768, out_features=3072, bias=True)\n","    (intermediate_act_fn): GELUActivation()\n","  )\n","  (output): RobertaOutput(\n","    (dense): Linear(in_features=3072, out_features=768, bias=True)\n","    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","    (dropout): Dropout(p=0.1, inplace=False)\n","  )\n",")\n","intermediate.encoder.layer.1.attention*******\n","RobertaAttention(\n","  (self): RobertaSelfAttention(\n","    (query): Linear(in_features=768, out_features=768, bias=True)\n","    (key): Linear(in_features=768, out_features=768, bias=True)\n","    (value): Linear(in_features=768, out_features=768, bias=True)\n","    (dropout): Dropout(p=0.1, inplace=False)\n","  )\n","  (output): RobertaSelfOutput(\n","    (dense): Linear(in_features=768, out_features=768, bias=True)\n","    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","    (dropout): Dropout(p=0.1, inplace=False)\n","  )\n",")\n","intermediate.encoder.layer.1.attention.self*******\n","RobertaSelfAttention(\n","  (query): Linear(in_features=768, out_features=768, bias=True)\n","  (key): Linear(in_features=768, out_features=768, bias=True)\n","  (value): Linear(in_features=768, out_features=768, bias=True)\n","  (dropout): Dropout(p=0.1, inplace=False)\n",")\n","intermediate.encoder.layer.1.attention.self.query*******\n","Linear(in_features=768, out_features=768, bias=True)\n","intermediate.encoder.layer.1.attention.self.key*******\n","Linear(in_features=768, out_features=768, bias=True)\n","intermediate.encoder.layer.1.attention.self.value*******\n","Linear(in_features=768, out_features=768, bias=True)\n","intermediate.encoder.layer.1.attention.self.dropout*******\n","Dropout(p=0.1, inplace=False)\n","intermediate.encoder.layer.1.attention.output*******\n","RobertaSelfOutput(\n","  (dense): Linear(in_features=768, out_features=768, bias=True)\n","  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","  (dropout): Dropout(p=0.1, inplace=False)\n",")\n","intermediate.encoder.layer.1.attention.output.dense*******\n","Linear(in_features=768, out_features=768, bias=True)\n","intermediate.encoder.layer.1.attention.output.LayerNorm*******\n","LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","intermediate.encoder.layer.1.attention.output.dropout*******\n","Dropout(p=0.1, inplace=False)\n","intermediate.encoder.layer.1.intermediate*******\n","RobertaIntermediate(\n","  (dense): Linear(in_features=768, out_features=3072, bias=True)\n","  (intermediate_act_fn): GELUActivation()\n",")\n","intermediate.encoder.layer.1.intermediate.dense*******\n","Linear(in_features=768, out_features=3072, bias=True)\n","intermediate.encoder.layer.1.intermediate.intermediate_act_fn*******\n","GELUActivation()\n","intermediate.encoder.layer.1.output*******\n","RobertaOutput(\n","  (dense): Linear(in_features=3072, out_features=768, bias=True)\n","  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","  (dropout): Dropout(p=0.1, inplace=False)\n",")\n","intermediate.encoder.layer.1.output.dense*******\n","Linear(in_features=3072, out_features=768, bias=True)\n","intermediate.encoder.layer.1.output.LayerNorm*******\n","LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","intermediate.encoder.layer.1.output.dropout*******\n","Dropout(p=0.1, inplace=False)\n","intermediate.encoder.layer.2*******\n","RobertaLayer(\n","  (attention): RobertaAttention(\n","    (self): RobertaSelfAttention(\n","      (query): Linear(in_features=768, out_features=768, bias=True)\n","      (key): Linear(in_features=768, out_features=768, bias=True)\n","      (value): Linear(in_features=768, out_features=768, bias=True)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (output): RobertaSelfOutput(\n","      (dense): Linear(in_features=768, out_features=768, bias=True)\n","      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","  )\n","  (intermediate): RobertaIntermediate(\n","    (dense): Linear(in_features=768, out_features=3072, bias=True)\n","    (intermediate_act_fn): GELUActivation()\n","  )\n","  (output): RobertaOutput(\n","    (dense): Linear(in_features=3072, out_features=768, bias=True)\n","    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","    (dropout): Dropout(p=0.1, inplace=False)\n","  )\n",")\n","intermediate.encoder.layer.2.attention*******\n","RobertaAttention(\n","  (self): RobertaSelfAttention(\n","    (query): Linear(in_features=768, out_features=768, bias=True)\n","    (key): Linear(in_features=768, out_features=768, bias=True)\n","    (value): Linear(in_features=768, out_features=768, bias=True)\n","    (dropout): Dropout(p=0.1, inplace=False)\n","  )\n","  (output): RobertaSelfOutput(\n","    (dense): Linear(in_features=768, out_features=768, bias=True)\n","    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","    (dropout): Dropout(p=0.1, inplace=False)\n","  )\n",")\n","intermediate.encoder.layer.2.attention.self*******\n","RobertaSelfAttention(\n","  (query): Linear(in_features=768, out_features=768, bias=True)\n","  (key): Linear(in_features=768, out_features=768, bias=True)\n","  (value): Linear(in_features=768, out_features=768, bias=True)\n","  (dropout): Dropout(p=0.1, inplace=False)\n",")\n","intermediate.encoder.layer.2.attention.self.query*******\n","Linear(in_features=768, out_features=768, bias=True)\n","intermediate.encoder.layer.2.attention.self.key*******\n","Linear(in_features=768, out_features=768, bias=True)\n","intermediate.encoder.layer.2.attention.self.value*******\n","Linear(in_features=768, out_features=768, bias=True)\n","intermediate.encoder.layer.2.attention.self.dropout*******\n","Dropout(p=0.1, inplace=False)\n","intermediate.encoder.layer.2.attention.output*******\n","RobertaSelfOutput(\n","  (dense): Linear(in_features=768, out_features=768, bias=True)\n","  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","  (dropout): Dropout(p=0.1, inplace=False)\n",")\n","intermediate.encoder.layer.2.attention.output.dense*******\n","Linear(in_features=768, out_features=768, bias=True)\n","intermediate.encoder.layer.2.attention.output.LayerNorm*******\n","LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","intermediate.encoder.layer.2.attention.output.dropout*******\n","Dropout(p=0.1, inplace=False)\n","intermediate.encoder.layer.2.intermediate*******\n","RobertaIntermediate(\n","  (dense): Linear(in_features=768, out_features=3072, bias=True)\n","  (intermediate_act_fn): GELUActivation()\n",")\n","intermediate.encoder.layer.2.intermediate.dense*******\n","Linear(in_features=768, out_features=3072, bias=True)\n","intermediate.encoder.layer.2.intermediate.intermediate_act_fn*******\n","GELUActivation()\n","intermediate.encoder.layer.2.output*******\n","RobertaOutput(\n","  (dense): Linear(in_features=3072, out_features=768, bias=True)\n","  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","  (dropout): Dropout(p=0.1, inplace=False)\n",")\n","intermediate.encoder.layer.2.output.dense*******\n","Linear(in_features=3072, out_features=768, bias=True)\n","intermediate.encoder.layer.2.output.LayerNorm*******\n","LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","intermediate.encoder.layer.2.output.dropout*******\n","Dropout(p=0.1, inplace=False)\n","intermediate.encoder.layer.3*******\n","RobertaLayer(\n","  (attention): RobertaAttention(\n","    (self): RobertaSelfAttention(\n","      (query): Linear(in_features=768, out_features=768, bias=True)\n","      (key): Linear(in_features=768, out_features=768, bias=True)\n","      (value): Linear(in_features=768, out_features=768, bias=True)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (output): RobertaSelfOutput(\n","      (dense): Linear(in_features=768, out_features=768, bias=True)\n","      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","  )\n","  (intermediate): RobertaIntermediate(\n","    (dense): Linear(in_features=768, out_features=3072, bias=True)\n","    (intermediate_act_fn): GELUActivation()\n","  )\n","  (output): RobertaOutput(\n","    (dense): Linear(in_features=3072, out_features=768, bias=True)\n","    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","    (dropout): Dropout(p=0.1, inplace=False)\n","  )\n",")\n","intermediate.encoder.layer.3.attention*******\n","RobertaAttention(\n","  (self): RobertaSelfAttention(\n","    (query): Linear(in_features=768, out_features=768, bias=True)\n","    (key): Linear(in_features=768, out_features=768, bias=True)\n","    (value): Linear(in_features=768, out_features=768, bias=True)\n","    (dropout): Dropout(p=0.1, inplace=False)\n","  )\n","  (output): RobertaSelfOutput(\n","    (dense): Linear(in_features=768, out_features=768, bias=True)\n","    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","    (dropout): Dropout(p=0.1, inplace=False)\n","  )\n",")\n","intermediate.encoder.layer.3.attention.self*******\n","RobertaSelfAttention(\n","  (query): Linear(in_features=768, out_features=768, bias=True)\n","  (key): Linear(in_features=768, out_features=768, bias=True)\n","  (value): Linear(in_features=768, out_features=768, bias=True)\n","  (dropout): Dropout(p=0.1, inplace=False)\n",")\n","intermediate.encoder.layer.3.attention.self.query*******\n","Linear(in_features=768, out_features=768, bias=True)\n","intermediate.encoder.layer.3.attention.self.key*******\n","Linear(in_features=768, out_features=768, bias=True)\n","intermediate.encoder.layer.3.attention.self.value*******\n","Linear(in_features=768, out_features=768, bias=True)\n","intermediate.encoder.layer.3.attention.self.dropout*******\n","Dropout(p=0.1, inplace=False)\n","intermediate.encoder.layer.3.attention.output*******\n","RobertaSelfOutput(\n","  (dense): Linear(in_features=768, out_features=768, bias=True)\n","  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","  (dropout): Dropout(p=0.1, inplace=False)\n",")\n","intermediate.encoder.layer.3.attention.output.dense*******\n","Linear(in_features=768, out_features=768, bias=True)\n","intermediate.encoder.layer.3.attention.output.LayerNorm*******\n","LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","intermediate.encoder.layer.3.attention.output.dropout*******\n","Dropout(p=0.1, inplace=False)\n","intermediate.encoder.layer.3.intermediate*******\n","RobertaIntermediate(\n","  (dense): Linear(in_features=768, out_features=3072, bias=True)\n","  (intermediate_act_fn): GELUActivation()\n",")\n","intermediate.encoder.layer.3.intermediate.dense*******\n","Linear(in_features=768, out_features=3072, bias=True)\n","intermediate.encoder.layer.3.intermediate.intermediate_act_fn*******\n","GELUActivation()\n","intermediate.encoder.layer.3.output*******\n","RobertaOutput(\n","  (dense): Linear(in_features=3072, out_features=768, bias=True)\n","  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","  (dropout): Dropout(p=0.1, inplace=False)\n",")\n","intermediate.encoder.layer.3.output.dense*******\n","Linear(in_features=3072, out_features=768, bias=True)\n","intermediate.encoder.layer.3.output.LayerNorm*******\n","LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","intermediate.encoder.layer.3.output.dropout*******\n","Dropout(p=0.1, inplace=False)\n","intermediate.encoder.layer.4*******\n","RobertaLayer(\n","  (attention): RobertaAttention(\n","    (self): RobertaSelfAttention(\n","      (query): Linear(in_features=768, out_features=768, bias=True)\n","      (key): Linear(in_features=768, out_features=768, bias=True)\n","      (value): Linear(in_features=768, out_features=768, bias=True)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (output): RobertaSelfOutput(\n","      (dense): Linear(in_features=768, out_features=768, bias=True)\n","      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","  )\n","  (intermediate): RobertaIntermediate(\n","    (dense): Linear(in_features=768, out_features=3072, bias=True)\n","    (intermediate_act_fn): GELUActivation()\n","  )\n","  (output): RobertaOutput(\n","    (dense): Linear(in_features=3072, out_features=768, bias=True)\n","    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","    (dropout): Dropout(p=0.1, inplace=False)\n","  )\n",")\n","intermediate.encoder.layer.4.attention*******\n","RobertaAttention(\n","  (self): RobertaSelfAttention(\n","    (query): Linear(in_features=768, out_features=768, bias=True)\n","    (key): Linear(in_features=768, out_features=768, bias=True)\n","    (value): Linear(in_features=768, out_features=768, bias=True)\n","    (dropout): Dropout(p=0.1, inplace=False)\n","  )\n","  (output): RobertaSelfOutput(\n","    (dense): Linear(in_features=768, out_features=768, bias=True)\n","    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","    (dropout): Dropout(p=0.1, inplace=False)\n","  )\n",")\n","intermediate.encoder.layer.4.attention.self*******\n","RobertaSelfAttention(\n","  (query): Linear(in_features=768, out_features=768, bias=True)\n","  (key): Linear(in_features=768, out_features=768, bias=True)\n","  (value): Linear(in_features=768, out_features=768, bias=True)\n","  (dropout): Dropout(p=0.1, inplace=False)\n",")\n","intermediate.encoder.layer.4.attention.self.query*******\n","Linear(in_features=768, out_features=768, bias=True)\n","intermediate.encoder.layer.4.attention.self.key*******\n","Linear(in_features=768, out_features=768, bias=True)\n","intermediate.encoder.layer.4.attention.self.value*******\n","Linear(in_features=768, out_features=768, bias=True)\n","intermediate.encoder.layer.4.attention.self.dropout*******\n","Dropout(p=0.1, inplace=False)\n","intermediate.encoder.layer.4.attention.output*******\n","RobertaSelfOutput(\n","  (dense): Linear(in_features=768, out_features=768, bias=True)\n","  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","  (dropout): Dropout(p=0.1, inplace=False)\n",")\n","intermediate.encoder.layer.4.attention.output.dense*******\n","Linear(in_features=768, out_features=768, bias=True)\n","intermediate.encoder.layer.4.attention.output.LayerNorm*******\n","LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","intermediate.encoder.layer.4.attention.output.dropout*******\n","Dropout(p=0.1, inplace=False)\n","intermediate.encoder.layer.4.intermediate*******\n","RobertaIntermediate(\n","  (dense): Linear(in_features=768, out_features=3072, bias=True)\n","  (intermediate_act_fn): GELUActivation()\n",")\n","intermediate.encoder.layer.4.intermediate.dense*******\n","Linear(in_features=768, out_features=3072, bias=True)\n","intermediate.encoder.layer.4.intermediate.intermediate_act_fn*******\n","GELUActivation()\n","intermediate.encoder.layer.4.output*******\n","RobertaOutput(\n","  (dense): Linear(in_features=3072, out_features=768, bias=True)\n","  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","  (dropout): Dropout(p=0.1, inplace=False)\n",")\n","intermediate.encoder.layer.4.output.dense*******\n","Linear(in_features=3072, out_features=768, bias=True)\n","intermediate.encoder.layer.4.output.LayerNorm*******\n","LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","intermediate.encoder.layer.4.output.dropout*******\n","Dropout(p=0.1, inplace=False)\n","intermediate.encoder.layer.5*******\n","RobertaLayer(\n","  (attention): RobertaAttention(\n","    (self): RobertaSelfAttention(\n","      (query): Linear(in_features=768, out_features=768, bias=True)\n","      (key): Linear(in_features=768, out_features=768, bias=True)\n","      (value): Linear(in_features=768, out_features=768, bias=True)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (output): RobertaSelfOutput(\n","      (dense): Linear(in_features=768, out_features=768, bias=True)\n","      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","  )\n","  (intermediate): RobertaIntermediate(\n","    (dense): Linear(in_features=768, out_features=3072, bias=True)\n","    (intermediate_act_fn): GELUActivation()\n","  )\n","  (output): RobertaOutput(\n","    (dense): Linear(in_features=3072, out_features=768, bias=True)\n","    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","    (dropout): Dropout(p=0.1, inplace=False)\n","  )\n",")\n","intermediate.encoder.layer.5.attention*******\n","RobertaAttention(\n","  (self): RobertaSelfAttention(\n","    (query): Linear(in_features=768, out_features=768, bias=True)\n","    (key): Linear(in_features=768, out_features=768, bias=True)\n","    (value): Linear(in_features=768, out_features=768, bias=True)\n","    (dropout): Dropout(p=0.1, inplace=False)\n","  )\n","  (output): RobertaSelfOutput(\n","    (dense): Linear(in_features=768, out_features=768, bias=True)\n","    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","    (dropout): Dropout(p=0.1, inplace=False)\n","  )\n",")\n","intermediate.encoder.layer.5.attention.self*******\n","RobertaSelfAttention(\n","  (query): Linear(in_features=768, out_features=768, bias=True)\n","  (key): Linear(in_features=768, out_features=768, bias=True)\n","  (value): Linear(in_features=768, out_features=768, bias=True)\n","  (dropout): Dropout(p=0.1, inplace=False)\n",")\n","intermediate.encoder.layer.5.attention.self.query*******\n","Linear(in_features=768, out_features=768, bias=True)\n","intermediate.encoder.layer.5.attention.self.key*******\n","Linear(in_features=768, out_features=768, bias=True)\n","intermediate.encoder.layer.5.attention.self.value*******\n","Linear(in_features=768, out_features=768, bias=True)\n","intermediate.encoder.layer.5.attention.self.dropout*******\n","Dropout(p=0.1, inplace=False)\n","intermediate.encoder.layer.5.attention.output*******\n","RobertaSelfOutput(\n","  (dense): Linear(in_features=768, out_features=768, bias=True)\n","  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","  (dropout): Dropout(p=0.1, inplace=False)\n",")\n","intermediate.encoder.layer.5.attention.output.dense*******\n","Linear(in_features=768, out_features=768, bias=True)\n","intermediate.encoder.layer.5.attention.output.LayerNorm*******\n","LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","intermediate.encoder.layer.5.attention.output.dropout*******\n","Dropout(p=0.1, inplace=False)\n","intermediate.encoder.layer.5.intermediate*******\n","RobertaIntermediate(\n","  (dense): Linear(in_features=768, out_features=3072, bias=True)\n","  (intermediate_act_fn): GELUActivation()\n",")\n","intermediate.encoder.layer.5.intermediate.dense*******\n","Linear(in_features=768, out_features=3072, bias=True)\n","intermediate.encoder.layer.5.intermediate.intermediate_act_fn*******\n","GELUActivation()\n","intermediate.encoder.layer.5.output*******\n","RobertaOutput(\n","  (dense): Linear(in_features=3072, out_features=768, bias=True)\n","  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","  (dropout): Dropout(p=0.1, inplace=False)\n",")\n","intermediate.encoder.layer.5.output.dense*******\n","Linear(in_features=3072, out_features=768, bias=True)\n","intermediate.encoder.layer.5.output.LayerNorm*******\n","LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","intermediate.encoder.layer.5.output.dropout*******\n","Dropout(p=0.1, inplace=False)\n","intermediate.encoder.layer.6*******\n","RobertaLayer(\n","  (attention): RobertaAttention(\n","    (self): RobertaSelfAttention(\n","      (query): Linear(in_features=768, out_features=768, bias=True)\n","      (key): Linear(in_features=768, out_features=768, bias=True)\n","      (value): Linear(in_features=768, out_features=768, bias=True)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (output): RobertaSelfOutput(\n","      (dense): Linear(in_features=768, out_features=768, bias=True)\n","      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","  )\n","  (intermediate): RobertaIntermediate(\n","    (dense): Linear(in_features=768, out_features=3072, bias=True)\n","    (intermediate_act_fn): GELUActivation()\n","  )\n","  (output): RobertaOutput(\n","    (dense): Linear(in_features=3072, out_features=768, bias=True)\n","    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","    (dropout): Dropout(p=0.1, inplace=False)\n","  )\n",")\n","intermediate.encoder.layer.6.attention*******\n","RobertaAttention(\n","  (self): RobertaSelfAttention(\n","    (query): Linear(in_features=768, out_features=768, bias=True)\n","    (key): Linear(in_features=768, out_features=768, bias=True)\n","    (value): Linear(in_features=768, out_features=768, bias=True)\n","    (dropout): Dropout(p=0.1, inplace=False)\n","  )\n","  (output): RobertaSelfOutput(\n","    (dense): Linear(in_features=768, out_features=768, bias=True)\n","    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","    (dropout): Dropout(p=0.1, inplace=False)\n","  )\n",")\n","intermediate.encoder.layer.6.attention.self*******\n","RobertaSelfAttention(\n","  (query): Linear(in_features=768, out_features=768, bias=True)\n","  (key): Linear(in_features=768, out_features=768, bias=True)\n","  (value): Linear(in_features=768, out_features=768, bias=True)\n","  (dropout): Dropout(p=0.1, inplace=False)\n",")\n","intermediate.encoder.layer.6.attention.self.query*******\n","Linear(in_features=768, out_features=768, bias=True)\n","intermediate.encoder.layer.6.attention.self.key*******\n","Linear(in_features=768, out_features=768, bias=True)\n","intermediate.encoder.layer.6.attention.self.value*******\n","Linear(in_features=768, out_features=768, bias=True)\n","intermediate.encoder.layer.6.attention.self.dropout*******\n","Dropout(p=0.1, inplace=False)\n","intermediate.encoder.layer.6.attention.output*******\n","RobertaSelfOutput(\n","  (dense): Linear(in_features=768, out_features=768, bias=True)\n","  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","  (dropout): Dropout(p=0.1, inplace=False)\n",")\n","intermediate.encoder.layer.6.attention.output.dense*******\n","Linear(in_features=768, out_features=768, bias=True)\n","intermediate.encoder.layer.6.attention.output.LayerNorm*******\n","LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","intermediate.encoder.layer.6.attention.output.dropout*******\n","Dropout(p=0.1, inplace=False)\n","intermediate.encoder.layer.6.intermediate*******\n","RobertaIntermediate(\n","  (dense): Linear(in_features=768, out_features=3072, bias=True)\n","  (intermediate_act_fn): GELUActivation()\n",")\n","intermediate.encoder.layer.6.intermediate.dense*******\n","Linear(in_features=768, out_features=3072, bias=True)\n","intermediate.encoder.layer.6.intermediate.intermediate_act_fn*******\n","GELUActivation()\n","intermediate.encoder.layer.6.output*******\n","RobertaOutput(\n","  (dense): Linear(in_features=3072, out_features=768, bias=True)\n","  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","  (dropout): Dropout(p=0.1, inplace=False)\n",")\n","intermediate.encoder.layer.6.output.dense*******\n","Linear(in_features=3072, out_features=768, bias=True)\n","intermediate.encoder.layer.6.output.LayerNorm*******\n","LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","intermediate.encoder.layer.6.output.dropout*******\n","Dropout(p=0.1, inplace=False)\n","intermediate.encoder.layer.7*******\n","RobertaLayer(\n","  (attention): RobertaAttention(\n","    (self): RobertaSelfAttention(\n","      (query): Linear(in_features=768, out_features=768, bias=True)\n","      (key): Linear(in_features=768, out_features=768, bias=True)\n","      (value): Linear(in_features=768, out_features=768, bias=True)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (output): RobertaSelfOutput(\n","      (dense): Linear(in_features=768, out_features=768, bias=True)\n","      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","  )\n","  (intermediate): RobertaIntermediate(\n","    (dense): Linear(in_features=768, out_features=3072, bias=True)\n","    (intermediate_act_fn): GELUActivation()\n","  )\n","  (output): RobertaOutput(\n","    (dense): Linear(in_features=3072, out_features=768, bias=True)\n","    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","    (dropout): Dropout(p=0.1, inplace=False)\n","  )\n",")\n","intermediate.encoder.layer.7.attention*******\n","RobertaAttention(\n","  (self): RobertaSelfAttention(\n","    (query): Linear(in_features=768, out_features=768, bias=True)\n","    (key): Linear(in_features=768, out_features=768, bias=True)\n","    (value): Linear(in_features=768, out_features=768, bias=True)\n","    (dropout): Dropout(p=0.1, inplace=False)\n","  )\n","  (output): RobertaSelfOutput(\n","    (dense): Linear(in_features=768, out_features=768, bias=True)\n","    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","    (dropout): Dropout(p=0.1, inplace=False)\n","  )\n",")\n","intermediate.encoder.layer.7.attention.self*******\n","RobertaSelfAttention(\n","  (query): Linear(in_features=768, out_features=768, bias=True)\n","  (key): Linear(in_features=768, out_features=768, bias=True)\n","  (value): Linear(in_features=768, out_features=768, bias=True)\n","  (dropout): Dropout(p=0.1, inplace=False)\n",")\n","intermediate.encoder.layer.7.attention.self.query*******\n","Linear(in_features=768, out_features=768, bias=True)\n","intermediate.encoder.layer.7.attention.self.key*******\n","Linear(in_features=768, out_features=768, bias=True)\n","intermediate.encoder.layer.7.attention.self.value*******\n","Linear(in_features=768, out_features=768, bias=True)\n","intermediate.encoder.layer.7.attention.self.dropout*******\n","Dropout(p=0.1, inplace=False)\n","intermediate.encoder.layer.7.attention.output*******\n","RobertaSelfOutput(\n","  (dense): Linear(in_features=768, out_features=768, bias=True)\n","  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","  (dropout): Dropout(p=0.1, inplace=False)\n",")\n","intermediate.encoder.layer.7.attention.output.dense*******\n","Linear(in_features=768, out_features=768, bias=True)\n","intermediate.encoder.layer.7.attention.output.LayerNorm*******\n","LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","intermediate.encoder.layer.7.attention.output.dropout*******\n","Dropout(p=0.1, inplace=False)\n","intermediate.encoder.layer.7.intermediate*******\n","RobertaIntermediate(\n","  (dense): Linear(in_features=768, out_features=3072, bias=True)\n","  (intermediate_act_fn): GELUActivation()\n",")\n","intermediate.encoder.layer.7.intermediate.dense*******\n","Linear(in_features=768, out_features=3072, bias=True)\n","intermediate.encoder.layer.7.intermediate.intermediate_act_fn*******\n","GELUActivation()\n","intermediate.encoder.layer.7.output*******\n","RobertaOutput(\n","  (dense): Linear(in_features=3072, out_features=768, bias=True)\n","  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","  (dropout): Dropout(p=0.1, inplace=False)\n",")\n","intermediate.encoder.layer.7.output.dense*******\n","Linear(in_features=3072, out_features=768, bias=True)\n","intermediate.encoder.layer.7.output.LayerNorm*******\n","LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","intermediate.encoder.layer.7.output.dropout*******\n","Dropout(p=0.1, inplace=False)\n","intermediate.encoder.layer.8*******\n","RobertaLayer(\n","  (attention): RobertaAttention(\n","    (self): RobertaSelfAttention(\n","      (query): Linear(in_features=768, out_features=768, bias=True)\n","      (key): Linear(in_features=768, out_features=768, bias=True)\n","      (value): Linear(in_features=768, out_features=768, bias=True)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (output): RobertaSelfOutput(\n","      (dense): Linear(in_features=768, out_features=768, bias=True)\n","      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","  )\n","  (intermediate): RobertaIntermediate(\n","    (dense): Linear(in_features=768, out_features=3072, bias=True)\n","    (intermediate_act_fn): GELUActivation()\n","  )\n","  (output): RobertaOutput(\n","    (dense): Linear(in_features=3072, out_features=768, bias=True)\n","    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","    (dropout): Dropout(p=0.1, inplace=False)\n","  )\n",")\n","intermediate.encoder.layer.8.attention*******\n","RobertaAttention(\n","  (self): RobertaSelfAttention(\n","    (query): Linear(in_features=768, out_features=768, bias=True)\n","    (key): Linear(in_features=768, out_features=768, bias=True)\n","    (value): Linear(in_features=768, out_features=768, bias=True)\n","    (dropout): Dropout(p=0.1, inplace=False)\n","  )\n","  (output): RobertaSelfOutput(\n","    (dense): Linear(in_features=768, out_features=768, bias=True)\n","    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","    (dropout): Dropout(p=0.1, inplace=False)\n","  )\n",")\n","intermediate.encoder.layer.8.attention.self*******\n","RobertaSelfAttention(\n","  (query): Linear(in_features=768, out_features=768, bias=True)\n","  (key): Linear(in_features=768, out_features=768, bias=True)\n","  (value): Linear(in_features=768, out_features=768, bias=True)\n","  (dropout): Dropout(p=0.1, inplace=False)\n",")\n","intermediate.encoder.layer.8.attention.self.query*******\n","Linear(in_features=768, out_features=768, bias=True)\n","intermediate.encoder.layer.8.attention.self.key*******\n","Linear(in_features=768, out_features=768, bias=True)\n","intermediate.encoder.layer.8.attention.self.value*******\n","Linear(in_features=768, out_features=768, bias=True)\n","intermediate.encoder.layer.8.attention.self.dropout*******\n","Dropout(p=0.1, inplace=False)\n","intermediate.encoder.layer.8.attention.output*******\n","RobertaSelfOutput(\n","  (dense): Linear(in_features=768, out_features=768, bias=True)\n","  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","  (dropout): Dropout(p=0.1, inplace=False)\n",")\n","intermediate.encoder.layer.8.attention.output.dense*******\n","Linear(in_features=768, out_features=768, bias=True)\n","intermediate.encoder.layer.8.attention.output.LayerNorm*******\n","LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","intermediate.encoder.layer.8.attention.output.dropout*******\n","Dropout(p=0.1, inplace=False)\n","intermediate.encoder.layer.8.intermediate*******\n","RobertaIntermediate(\n","  (dense): Linear(in_features=768, out_features=3072, bias=True)\n","  (intermediate_act_fn): GELUActivation()\n",")\n","intermediate.encoder.layer.8.intermediate.dense*******\n","Linear(in_features=768, out_features=3072, bias=True)\n","intermediate.encoder.layer.8.intermediate.intermediate_act_fn*******\n","GELUActivation()\n","intermediate.encoder.layer.8.output*******\n","RobertaOutput(\n","  (dense): Linear(in_features=3072, out_features=768, bias=True)\n","  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","  (dropout): Dropout(p=0.1, inplace=False)\n",")\n","intermediate.encoder.layer.8.output.dense*******\n","Linear(in_features=3072, out_features=768, bias=True)\n","intermediate.encoder.layer.8.output.LayerNorm*******\n","LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","intermediate.encoder.layer.8.output.dropout*******\n","Dropout(p=0.1, inplace=False)\n","intermediate.encoder.layer.9*******\n","RobertaLayer(\n","  (attention): RobertaAttention(\n","    (self): RobertaSelfAttention(\n","      (query): Linear(in_features=768, out_features=768, bias=True)\n","      (key): Linear(in_features=768, out_features=768, bias=True)\n","      (value): Linear(in_features=768, out_features=768, bias=True)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (output): RobertaSelfOutput(\n","      (dense): Linear(in_features=768, out_features=768, bias=True)\n","      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","  )\n","  (intermediate): RobertaIntermediate(\n","    (dense): Linear(in_features=768, out_features=3072, bias=True)\n","    (intermediate_act_fn): GELUActivation()\n","  )\n","  (output): RobertaOutput(\n","    (dense): Linear(in_features=3072, out_features=768, bias=True)\n","    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","    (dropout): Dropout(p=0.1, inplace=False)\n","  )\n",")\n","intermediate.encoder.layer.9.attention*******\n","RobertaAttention(\n","  (self): RobertaSelfAttention(\n","    (query): Linear(in_features=768, out_features=768, bias=True)\n","    (key): Linear(in_features=768, out_features=768, bias=True)\n","    (value): Linear(in_features=768, out_features=768, bias=True)\n","    (dropout): Dropout(p=0.1, inplace=False)\n","  )\n","  (output): RobertaSelfOutput(\n","    (dense): Linear(in_features=768, out_features=768, bias=True)\n","    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","    (dropout): Dropout(p=0.1, inplace=False)\n","  )\n",")\n","intermediate.encoder.layer.9.attention.self*******\n","RobertaSelfAttention(\n","  (query): Linear(in_features=768, out_features=768, bias=True)\n","  (key): Linear(in_features=768, out_features=768, bias=True)\n","  (value): Linear(in_features=768, out_features=768, bias=True)\n","  (dropout): Dropout(p=0.1, inplace=False)\n",")\n","intermediate.encoder.layer.9.attention.self.query*******\n","Linear(in_features=768, out_features=768, bias=True)\n","intermediate.encoder.layer.9.attention.self.key*******\n","Linear(in_features=768, out_features=768, bias=True)\n","intermediate.encoder.layer.9.attention.self.value*******\n","Linear(in_features=768, out_features=768, bias=True)\n","intermediate.encoder.layer.9.attention.self.dropout*******\n","Dropout(p=0.1, inplace=False)\n","intermediate.encoder.layer.9.attention.output*******\n","RobertaSelfOutput(\n","  (dense): Linear(in_features=768, out_features=768, bias=True)\n","  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","  (dropout): Dropout(p=0.1, inplace=False)\n",")\n","intermediate.encoder.layer.9.attention.output.dense*******\n","Linear(in_features=768, out_features=768, bias=True)\n","intermediate.encoder.layer.9.attention.output.LayerNorm*******\n","LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","intermediate.encoder.layer.9.attention.output.dropout*******\n","Dropout(p=0.1, inplace=False)\n","intermediate.encoder.layer.9.intermediate*******\n","RobertaIntermediate(\n","  (dense): Linear(in_features=768, out_features=3072, bias=True)\n","  (intermediate_act_fn): GELUActivation()\n",")\n","intermediate.encoder.layer.9.intermediate.dense*******\n","Linear(in_features=768, out_features=3072, bias=True)\n","intermediate.encoder.layer.9.intermediate.intermediate_act_fn*******\n","GELUActivation()\n","intermediate.encoder.layer.9.output*******\n","RobertaOutput(\n","  (dense): Linear(in_features=3072, out_features=768, bias=True)\n","  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","  (dropout): Dropout(p=0.1, inplace=False)\n",")\n","intermediate.encoder.layer.9.output.dense*******\n","Linear(in_features=3072, out_features=768, bias=True)\n","intermediate.encoder.layer.9.output.LayerNorm*******\n","LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","intermediate.encoder.layer.9.output.dropout*******\n","Dropout(p=0.1, inplace=False)\n","intermediate.encoder.layer.10*******\n","RobertaLayer(\n","  (attention): RobertaAttention(\n","    (self): RobertaSelfAttention(\n","      (query): Linear(in_features=768, out_features=768, bias=True)\n","      (key): Linear(in_features=768, out_features=768, bias=True)\n","      (value): Linear(in_features=768, out_features=768, bias=True)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (output): RobertaSelfOutput(\n","      (dense): Linear(in_features=768, out_features=768, bias=True)\n","      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","  )\n","  (intermediate): RobertaIntermediate(\n","    (dense): Linear(in_features=768, out_features=3072, bias=True)\n","    (intermediate_act_fn): GELUActivation()\n","  )\n","  (output): RobertaOutput(\n","    (dense): Linear(in_features=3072, out_features=768, bias=True)\n","    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","    (dropout): Dropout(p=0.1, inplace=False)\n","  )\n",")\n","intermediate.encoder.layer.10.attention*******\n","RobertaAttention(\n","  (self): RobertaSelfAttention(\n","    (query): Linear(in_features=768, out_features=768, bias=True)\n","    (key): Linear(in_features=768, out_features=768, bias=True)\n","    (value): Linear(in_features=768, out_features=768, bias=True)\n","    (dropout): Dropout(p=0.1, inplace=False)\n","  )\n","  (output): RobertaSelfOutput(\n","    (dense): Linear(in_features=768, out_features=768, bias=True)\n","    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","    (dropout): Dropout(p=0.1, inplace=False)\n","  )\n",")\n","intermediate.encoder.layer.10.attention.self*******\n","RobertaSelfAttention(\n","  (query): Linear(in_features=768, out_features=768, bias=True)\n","  (key): Linear(in_features=768, out_features=768, bias=True)\n","  (value): Linear(in_features=768, out_features=768, bias=True)\n","  (dropout): Dropout(p=0.1, inplace=False)\n",")\n","intermediate.encoder.layer.10.attention.self.query*******\n","Linear(in_features=768, out_features=768, bias=True)\n","intermediate.encoder.layer.10.attention.self.key*******\n","Linear(in_features=768, out_features=768, bias=True)\n","intermediate.encoder.layer.10.attention.self.value*******\n","Linear(in_features=768, out_features=768, bias=True)\n","intermediate.encoder.layer.10.attention.self.dropout*******\n","Dropout(p=0.1, inplace=False)\n","intermediate.encoder.layer.10.attention.output*******\n","RobertaSelfOutput(\n","  (dense): Linear(in_features=768, out_features=768, bias=True)\n","  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","  (dropout): Dropout(p=0.1, inplace=False)\n",")\n","intermediate.encoder.layer.10.attention.output.dense*******\n","Linear(in_features=768, out_features=768, bias=True)\n","intermediate.encoder.layer.10.attention.output.LayerNorm*******\n","LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","intermediate.encoder.layer.10.attention.output.dropout*******\n","Dropout(p=0.1, inplace=False)\n","intermediate.encoder.layer.10.intermediate*******\n","RobertaIntermediate(\n","  (dense): Linear(in_features=768, out_features=3072, bias=True)\n","  (intermediate_act_fn): GELUActivation()\n",")\n","intermediate.encoder.layer.10.intermediate.dense*******\n","Linear(in_features=768, out_features=3072, bias=True)\n","intermediate.encoder.layer.10.intermediate.intermediate_act_fn*******\n","GELUActivation()\n","intermediate.encoder.layer.10.output*******\n","RobertaOutput(\n","  (dense): Linear(in_features=3072, out_features=768, bias=True)\n","  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","  (dropout): Dropout(p=0.1, inplace=False)\n",")\n","intermediate.encoder.layer.10.output.dense*******\n","Linear(in_features=3072, out_features=768, bias=True)\n","intermediate.encoder.layer.10.output.LayerNorm*******\n","LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","intermediate.encoder.layer.10.output.dropout*******\n","Dropout(p=0.1, inplace=False)\n","intermediate.encoder.layer.11*******\n","RobertaLayer(\n","  (attention): RobertaAttention(\n","    (self): RobertaSelfAttention(\n","      (query): Linear(in_features=768, out_features=768, bias=True)\n","      (key): Linear(in_features=768, out_features=768, bias=True)\n","      (value): Linear(in_features=768, out_features=768, bias=True)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (output): RobertaSelfOutput(\n","      (dense): Linear(in_features=768, out_features=768, bias=True)\n","      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","  )\n","  (intermediate): RobertaIntermediate(\n","    (dense): Linear(in_features=768, out_features=3072, bias=True)\n","    (intermediate_act_fn): GELUActivation()\n","  )\n","  (output): RobertaOutput(\n","    (dense): Linear(in_features=3072, out_features=768, bias=True)\n","    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","    (dropout): Dropout(p=0.1, inplace=False)\n","  )\n",")\n","intermediate.encoder.layer.11.attention*******\n","RobertaAttention(\n","  (self): RobertaSelfAttention(\n","    (query): Linear(in_features=768, out_features=768, bias=True)\n","    (key): Linear(in_features=768, out_features=768, bias=True)\n","    (value): Linear(in_features=768, out_features=768, bias=True)\n","    (dropout): Dropout(p=0.1, inplace=False)\n","  )\n","  (output): RobertaSelfOutput(\n","    (dense): Linear(in_features=768, out_features=768, bias=True)\n","    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","    (dropout): Dropout(p=0.1, inplace=False)\n","  )\n",")\n","intermediate.encoder.layer.11.attention.self*******\n","RobertaSelfAttention(\n","  (query): Linear(in_features=768, out_features=768, bias=True)\n","  (key): Linear(in_features=768, out_features=768, bias=True)\n","  (value): Linear(in_features=768, out_features=768, bias=True)\n","  (dropout): Dropout(p=0.1, inplace=False)\n",")\n","intermediate.encoder.layer.11.attention.self.query*******\n","Linear(in_features=768, out_features=768, bias=True)\n","intermediate.encoder.layer.11.attention.self.key*******\n","Linear(in_features=768, out_features=768, bias=True)\n","intermediate.encoder.layer.11.attention.self.value*******\n","Linear(in_features=768, out_features=768, bias=True)\n","intermediate.encoder.layer.11.attention.self.dropout*******\n","Dropout(p=0.1, inplace=False)\n","intermediate.encoder.layer.11.attention.output*******\n","RobertaSelfOutput(\n","  (dense): Linear(in_features=768, out_features=768, bias=True)\n","  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","  (dropout): Dropout(p=0.1, inplace=False)\n",")\n","intermediate.encoder.layer.11.attention.output.dense*******\n","Linear(in_features=768, out_features=768, bias=True)\n","intermediate.encoder.layer.11.attention.output.LayerNorm*******\n","LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","intermediate.encoder.layer.11.attention.output.dropout*******\n","Dropout(p=0.1, inplace=False)\n","intermediate.encoder.layer.11.intermediate*******\n","RobertaIntermediate(\n","  (dense): Linear(in_features=768, out_features=3072, bias=True)\n","  (intermediate_act_fn): GELUActivation()\n",")\n","intermediate.encoder.layer.11.intermediate.dense*******\n","Linear(in_features=768, out_features=3072, bias=True)\n","intermediate.encoder.layer.11.intermediate.intermediate_act_fn*******\n","GELUActivation()\n","intermediate.encoder.layer.11.output*******\n","RobertaOutput(\n","  (dense): Linear(in_features=3072, out_features=768, bias=True)\n","  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","  (dropout): Dropout(p=0.1, inplace=False)\n",")\n","intermediate.encoder.layer.11.output.dense*******\n","Linear(in_features=3072, out_features=768, bias=True)\n","intermediate.encoder.layer.11.output.LayerNorm*******\n","LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","intermediate.encoder.layer.11.output.dropout*******\n","Dropout(p=0.1, inplace=False)\n","intermediate.pooler*******\n","RobertaPooler(\n","  (dense): Linear(in_features=768, out_features=768, bias=True)\n","  (activation): Tanh()\n",")\n","intermediate.pooler.dense*******\n","Linear(in_features=768, out_features=768, bias=True)\n","intermediate.pooler.activation*******\n","Tanh()\n","span_info_collect*******\n","SICModel(\n","  (W_1): Linear(in_features=768, out_features=768, bias=True)\n","  (W_2): Linear(in_features=768, out_features=768, bias=True)\n","  (W_3): Linear(in_features=768, out_features=768, bias=True)\n","  (W_4): Linear(in_features=768, out_features=768, bias=True)\n",")\n","span_info_collect.W_1*******\n","Linear(in_features=768, out_features=768, bias=True)\n","span_info_collect.W_2*******\n","Linear(in_features=768, out_features=768, bias=True)\n","span_info_collect.W_3*******\n","Linear(in_features=768, out_features=768, bias=True)\n","span_info_collect.W_4*******\n","Linear(in_features=768, out_features=768, bias=True)\n","interpretation*******\n","InterpretationModel(\n","  (h_t): Linear(in_features=768, out_features=1, bias=True)\n",")\n","interpretation.h_t*******\n","Linear(in_features=768, out_features=1, bias=True)\n","output*******\n","Linear(in_features=768, out_features=2, bias=True)\n"]}],"source":["# Xuất các layer của mô hình\n","for name, module in model.named_modules():\n","    print(name, end=\"*******\\n\")\n","    print(module)\n","    # print(name)"]},{"cell_type":"code","execution_count":25,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":28,"status":"ok","timestamp":1698906108413,"user":{"displayName":"Anh Đức Bùi","userId":"02440975349884324274"},"user_tz":-420},"id":"vr6qwdBX7W7t","outputId":"542adc92-99ed-4b64-8535-4b64ef774681"},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","intermediate\n","intermediate.embeddings\n","intermediate.embeddings.word_embeddings\n","intermediate.embeddings.position_embeddings\n","intermediate.embeddings.token_type_embeddings\n","intermediate.embeddings.LayerNorm\n","intermediate.embeddings.dropout\n","intermediate.encoder\n","intermediate.encoder.layer\n","intermediate.encoder.layer.0\n","intermediate.encoder.layer.0.attention\n","intermediate.encoder.layer.0.attention.self\n","intermediate.encoder.layer.0.attention.self.query\n","intermediate.encoder.layer.0.attention.self.key\n","intermediate.encoder.layer.0.attention.self.value\n","intermediate.encoder.layer.0.attention.self.dropout\n","intermediate.encoder.layer.0.attention.output\n","intermediate.encoder.layer.0.attention.output.dense\n","intermediate.encoder.layer.0.attention.output.LayerNorm\n","intermediate.encoder.layer.0.attention.output.dropout\n","intermediate.encoder.layer.0.intermediate\n","intermediate.encoder.layer.0.intermediate.dense\n","intermediate.encoder.layer.0.intermediate.intermediate_act_fn\n","intermediate.encoder.layer.0.output\n","intermediate.encoder.layer.0.output.dense\n","intermediate.encoder.layer.0.output.LayerNorm\n","intermediate.encoder.layer.0.output.dropout\n","intermediate.encoder.layer.1\n","intermediate.encoder.layer.1.attention\n","intermediate.encoder.layer.1.attention.self\n","intermediate.encoder.layer.1.attention.self.query\n","intermediate.encoder.layer.1.attention.self.key\n","intermediate.encoder.layer.1.attention.self.value\n","intermediate.encoder.layer.1.attention.self.dropout\n","intermediate.encoder.layer.1.attention.output\n","intermediate.encoder.layer.1.attention.output.dense\n","intermediate.encoder.layer.1.attention.output.LayerNorm\n","intermediate.encoder.layer.1.attention.output.dropout\n","intermediate.encoder.layer.1.intermediate\n","intermediate.encoder.layer.1.intermediate.dense\n","intermediate.encoder.layer.1.intermediate.intermediate_act_fn\n","intermediate.encoder.layer.1.output\n","intermediate.encoder.layer.1.output.dense\n","intermediate.encoder.layer.1.output.LayerNorm\n","intermediate.encoder.layer.1.output.dropout\n","intermediate.encoder.layer.2\n","intermediate.encoder.layer.2.attention\n","intermediate.encoder.layer.2.attention.self\n","intermediate.encoder.layer.2.attention.self.query\n","intermediate.encoder.layer.2.attention.self.key\n","intermediate.encoder.layer.2.attention.self.value\n","intermediate.encoder.layer.2.attention.self.dropout\n","intermediate.encoder.layer.2.attention.output\n","intermediate.encoder.layer.2.attention.output.dense\n","intermediate.encoder.layer.2.attention.output.LayerNorm\n","intermediate.encoder.layer.2.attention.output.dropout\n","intermediate.encoder.layer.2.intermediate\n","intermediate.encoder.layer.2.intermediate.dense\n","intermediate.encoder.layer.2.intermediate.intermediate_act_fn\n","intermediate.encoder.layer.2.output\n","intermediate.encoder.layer.2.output.dense\n","intermediate.encoder.layer.2.output.LayerNorm\n","intermediate.encoder.layer.2.output.dropout\n","intermediate.encoder.layer.3\n","intermediate.encoder.layer.3.attention\n","intermediate.encoder.layer.3.attention.self\n","intermediate.encoder.layer.3.attention.self.query\n","intermediate.encoder.layer.3.attention.self.key\n","intermediate.encoder.layer.3.attention.self.value\n","intermediate.encoder.layer.3.attention.self.dropout\n","intermediate.encoder.layer.3.attention.output\n","intermediate.encoder.layer.3.attention.output.dense\n","intermediate.encoder.layer.3.attention.output.LayerNorm\n","intermediate.encoder.layer.3.attention.output.dropout\n","intermediate.encoder.layer.3.intermediate\n","intermediate.encoder.layer.3.intermediate.dense\n","intermediate.encoder.layer.3.intermediate.intermediate_act_fn\n","intermediate.encoder.layer.3.output\n","intermediate.encoder.layer.3.output.dense\n","intermediate.encoder.layer.3.output.LayerNorm\n","intermediate.encoder.layer.3.output.dropout\n","intermediate.encoder.layer.4\n","intermediate.encoder.layer.4.attention\n","intermediate.encoder.layer.4.attention.self\n","intermediate.encoder.layer.4.attention.self.query\n","intermediate.encoder.layer.4.attention.self.key\n","intermediate.encoder.layer.4.attention.self.value\n","intermediate.encoder.layer.4.attention.self.dropout\n","intermediate.encoder.layer.4.attention.output\n","intermediate.encoder.layer.4.attention.output.dense\n","intermediate.encoder.layer.4.attention.output.LayerNorm\n","intermediate.encoder.layer.4.attention.output.dropout\n","intermediate.encoder.layer.4.intermediate\n","intermediate.encoder.layer.4.intermediate.dense\n","intermediate.encoder.layer.4.intermediate.intermediate_act_fn\n","intermediate.encoder.layer.4.output\n","intermediate.encoder.layer.4.output.dense\n","intermediate.encoder.layer.4.output.LayerNorm\n","intermediate.encoder.layer.4.output.dropout\n","intermediate.encoder.layer.5\n","intermediate.encoder.layer.5.attention\n","intermediate.encoder.layer.5.attention.self\n","intermediate.encoder.layer.5.attention.self.query\n","intermediate.encoder.layer.5.attention.self.key\n","intermediate.encoder.layer.5.attention.self.value\n","intermediate.encoder.layer.5.attention.self.dropout\n","intermediate.encoder.layer.5.attention.output\n","intermediate.encoder.layer.5.attention.output.dense\n","intermediate.encoder.layer.5.attention.output.LayerNorm\n","intermediate.encoder.layer.5.attention.output.dropout\n","intermediate.encoder.layer.5.intermediate\n","intermediate.encoder.layer.5.intermediate.dense\n","intermediate.encoder.layer.5.intermediate.intermediate_act_fn\n","intermediate.encoder.layer.5.output\n","intermediate.encoder.layer.5.output.dense\n","intermediate.encoder.layer.5.output.LayerNorm\n","intermediate.encoder.layer.5.output.dropout\n","intermediate.encoder.layer.6\n","intermediate.encoder.layer.6.attention\n","intermediate.encoder.layer.6.attention.self\n","intermediate.encoder.layer.6.attention.self.query\n","intermediate.encoder.layer.6.attention.self.key\n","intermediate.encoder.layer.6.attention.self.value\n","intermediate.encoder.layer.6.attention.self.dropout\n","intermediate.encoder.layer.6.attention.output\n","intermediate.encoder.layer.6.attention.output.dense\n","intermediate.encoder.layer.6.attention.output.LayerNorm\n","intermediate.encoder.layer.6.attention.output.dropout\n","intermediate.encoder.layer.6.intermediate\n","intermediate.encoder.layer.6.intermediate.dense\n","intermediate.encoder.layer.6.intermediate.intermediate_act_fn\n","intermediate.encoder.layer.6.output\n","intermediate.encoder.layer.6.output.dense\n","intermediate.encoder.layer.6.output.LayerNorm\n","intermediate.encoder.layer.6.output.dropout\n","intermediate.encoder.layer.7\n","intermediate.encoder.layer.7.attention\n","intermediate.encoder.layer.7.attention.self\n","intermediate.encoder.layer.7.attention.self.query\n","intermediate.encoder.layer.7.attention.self.key\n","intermediate.encoder.layer.7.attention.self.value\n","intermediate.encoder.layer.7.attention.self.dropout\n","intermediate.encoder.layer.7.attention.output\n","intermediate.encoder.layer.7.attention.output.dense\n","intermediate.encoder.layer.7.attention.output.LayerNorm\n","intermediate.encoder.layer.7.attention.output.dropout\n","intermediate.encoder.layer.7.intermediate\n","intermediate.encoder.layer.7.intermediate.dense\n","intermediate.encoder.layer.7.intermediate.intermediate_act_fn\n","intermediate.encoder.layer.7.output\n","intermediate.encoder.layer.7.output.dense\n","intermediate.encoder.layer.7.output.LayerNorm\n","intermediate.encoder.layer.7.output.dropout\n","intermediate.encoder.layer.8\n","intermediate.encoder.layer.8.attention\n","intermediate.encoder.layer.8.attention.self\n","intermediate.encoder.layer.8.attention.self.query\n","intermediate.encoder.layer.8.attention.self.key\n","intermediate.encoder.layer.8.attention.self.value\n","intermediate.encoder.layer.8.attention.self.dropout\n","intermediate.encoder.layer.8.attention.output\n","intermediate.encoder.layer.8.attention.output.dense\n","intermediate.encoder.layer.8.attention.output.LayerNorm\n","intermediate.encoder.layer.8.attention.output.dropout\n","intermediate.encoder.layer.8.intermediate\n","intermediate.encoder.layer.8.intermediate.dense\n","intermediate.encoder.layer.8.intermediate.intermediate_act_fn\n","intermediate.encoder.layer.8.output\n","intermediate.encoder.layer.8.output.dense\n","intermediate.encoder.layer.8.output.LayerNorm\n","intermediate.encoder.layer.8.output.dropout\n","intermediate.encoder.layer.9\n","intermediate.encoder.layer.9.attention\n","intermediate.encoder.layer.9.attention.self\n","intermediate.encoder.layer.9.attention.self.query\n","intermediate.encoder.layer.9.attention.self.key\n","intermediate.encoder.layer.9.attention.self.value\n","intermediate.encoder.layer.9.attention.self.dropout\n","intermediate.encoder.layer.9.attention.output\n","intermediate.encoder.layer.9.attention.output.dense\n","intermediate.encoder.layer.9.attention.output.LayerNorm\n","intermediate.encoder.layer.9.attention.output.dropout\n","intermediate.encoder.layer.9.intermediate\n","intermediate.encoder.layer.9.intermediate.dense\n","intermediate.encoder.layer.9.intermediate.intermediate_act_fn\n","intermediate.encoder.layer.9.output\n","intermediate.encoder.layer.9.output.dense\n","intermediate.encoder.layer.9.output.LayerNorm\n","intermediate.encoder.layer.9.output.dropout\n","intermediate.encoder.layer.10\n","intermediate.encoder.layer.10.attention\n","intermediate.encoder.layer.10.attention.self\n","intermediate.encoder.layer.10.attention.self.query\n","intermediate.encoder.layer.10.attention.self.key\n","intermediate.encoder.layer.10.attention.self.value\n","intermediate.encoder.layer.10.attention.self.dropout\n","intermediate.encoder.layer.10.attention.output\n","intermediate.encoder.layer.10.attention.output.dense\n","intermediate.encoder.layer.10.attention.output.LayerNorm\n","intermediate.encoder.layer.10.attention.output.dropout\n","intermediate.encoder.layer.10.intermediate\n","intermediate.encoder.layer.10.intermediate.dense\n","intermediate.encoder.layer.10.intermediate.intermediate_act_fn\n","intermediate.encoder.layer.10.output\n","intermediate.encoder.layer.10.output.dense\n","intermediate.encoder.layer.10.output.LayerNorm\n","intermediate.encoder.layer.10.output.dropout\n","intermediate.encoder.layer.11\n","intermediate.encoder.layer.11.attention\n","intermediate.encoder.layer.11.attention.self\n","intermediate.encoder.layer.11.attention.self.query\n","intermediate.encoder.layer.11.attention.self.key\n","intermediate.encoder.layer.11.attention.self.value\n","intermediate.encoder.layer.11.attention.self.dropout\n","intermediate.encoder.layer.11.attention.output\n","intermediate.encoder.layer.11.attention.output.dense\n","intermediate.encoder.layer.11.attention.output.LayerNorm\n","intermediate.encoder.layer.11.attention.output.dropout\n","intermediate.encoder.layer.11.intermediate\n","intermediate.encoder.layer.11.intermediate.dense\n","intermediate.encoder.layer.11.intermediate.intermediate_act_fn\n","intermediate.encoder.layer.11.output\n","intermediate.encoder.layer.11.output.dense\n","intermediate.encoder.layer.11.output.LayerNorm\n","intermediate.encoder.layer.11.output.dropout\n","intermediate.pooler\n","intermediate.pooler.dense\n","intermediate.pooler.activation\n","span_info_collect\n","span_info_collect.W_1\n","span_info_collect.W_2\n","span_info_collect.W_3\n","span_info_collect.W_4\n","interpretation\n","interpretation.h_t\n","output\n"]}],"source":["# Xuất các layer của mô hình\n","for name, module in model.named_modules():\n","\n","    print(name)"]},{"cell_type":"code","execution_count":26,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":15,"status":"ok","timestamp":1698906108413,"user":{"displayName":"Anh Đức Bùi","userId":"02440975349884324274"},"user_tz":-420},"id":"sB4KbM4n39iB","outputId":"53791456-ee98-434b-8e9a-06126f9ffcbd"},"outputs":[{"name":"stdout","output_type":"stream","text":["Số lượng tham số huấn luyện của mô hình Transformer: 137362947\n"]}],"source":["# Đếm số lượng tham số huấn luyện được\n","trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n","\n","print(\"Số lượng tham số huấn luyện của mô hình Transformer:\", trainable_params)"]},{"cell_type":"code","execution_count":27,"metadata":{"executionInfo":{"elapsed":13,"status":"ok","timestamp":1698906108414,"user":{"displayName":"Anh Đức Bùi","userId":"02440975349884324274"},"user_tz":-420},"id":"gJcQCvZl7_5r"},"outputs":[],"source":["\n","# Đóng băng các lớp trước layer11.output\n","layer11_found = False\n","for name, param in model.named_parameters():\n","    if name.startswith(\"intermediate.encoder.layer.11.output\"):\n","        layer11_found = True\n","    if not layer11_found:\n","        param.requires_grad_(False)"]},{"cell_type":"code","execution_count":28,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1698906109060,"user":{"displayName":"Anh Đức Bùi","userId":"02440975349884324274"},"user_tz":-420},"id":"YKyaUKO38CZq","outputId":"e7f0a2bd-39d3-4eb5-855f-1a51afa2d119"},"outputs":[{"name":"stdout","output_type":"stream","text":["Số lượng tham số huấn luyện của mô hình Transformer: 5316867\n"]}],"source":["# Đếm số lượng tham số huấn luyện được\n","trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n","\n","print(\"Số lượng tham số huấn luyện của mô hình Transformer:\", trainable_params)"]},{"cell_type":"code","execution_count":29,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":168,"referenced_widgets":["54fe938affc94b1091e300fa005b416f","ccebef5e53f4445fafcca8ac5f25f313","20fd13d9346d46d2bc536d6beb303b2c","f556b3ee62ab41db8e23d225423d15b4","c5aee511ed77462badc2084f2bdc3173","9e96ed983ce045f1ac00fedbbb9a47c1","b1e33d0b5c7542dd9e028432bf3cd0bf","ff88b2db12c94152979110eed88e6a88","de65fc6e1dad482fb047d8d13a889167","4ac8183d982943b983635358d416e67a","933f197010db41b2a4528f630a3a0c11","55d6853bc854436ea85bb2fe6d2ac444","27a5257ad0dd4f2fbded6a7a0092542f","450ca25852934038815c4507678e7960","6b268b0211d543ddbd33e81ded423d1a","9013ec23f4be4310a3257e7821ac21c2","7ace8cab1a8f405d93fb50a2a902edb0","66cfc1ebd3be44dabc2a45fd6818545b","95b01ecce14a4338b9714ebeed252386","c1a1f545fb4c456c8fcc23b88c70fc16","26543a0fa8154ab89aeb609432daf7c3","942d24158cb541edb8342b66156e246d","27c097839a8f4befab40d55830ff5bda","8e771d9ec13a4a91b98fbf1a2e9c4c15","72c6a9d448ed4ab684ce9349f2844e71","bec994eba6d7404f9e3563a2a00f6c1b","8ce7f75193a744d9aafa624322c5bd26","87b41e0663444295a7bb24a9e7528832","d71423216571427da86d93d4b7f07088","01122d2fca3b422c893fe45b2f9defd0","c2eb568196684ed992f8666e8374354f","0e7d3d33788c4f75a29d6b59967ccbe8","a525f1ffe7cc428abefb2fc494419ef4"]},"executionInfo":{"elapsed":5649,"status":"ok","timestamp":1698906114707,"user":{"displayName":"Anh Đức Bùi","userId":"02440975349884324274"},"user_tz":-420},"id":"wGo60taB4tYa","outputId":"b81409f2-5acb-432a-d3be-e8a4174810d8"},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of RobertaModel were not initialized from the model checkpoint at vinai/phobert-base-v2 and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"]}],"source":["import torch\n","from transformers import AutoModel, AutoTokenizer\n","\n","phobert = AutoModel.from_pretrained(\"vinai/phobert-base-v2\")\n","tokenizer = AutoTokenizer.from_pretrained(\"vinai/phobert-base-v2\")"]},{"cell_type":"code","execution_count":30,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1698906114708,"user":{"displayName":"Anh Đức Bùi","userId":"02440975349884324274"},"user_tz":-420},"id":"0nsy9TWz4ucA","outputId":"da75e270-0259-4a4b-a68a-904ad50db970"},"outputs":[{"data":{"text/plain":["RobertaModel(\n","  (embeddings): RobertaEmbeddings(\n","    (word_embeddings): Embedding(64001, 768, padding_idx=1)\n","    (position_embeddings): Embedding(258, 768, padding_idx=1)\n","    (token_type_embeddings): Embedding(1, 768)\n","    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","    (dropout): Dropout(p=0.1, inplace=False)\n","  )\n","  (encoder): RobertaEncoder(\n","    (layer): ModuleList(\n","      (0-11): 12 x RobertaLayer(\n","        (attention): RobertaAttention(\n","          (self): RobertaSelfAttention(\n","            (query): Linear(in_features=768, out_features=768, bias=True)\n","            (key): Linear(in_features=768, out_features=768, bias=True)\n","            (value): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): RobertaSelfOutput(\n","            (dense): Linear(in_features=768, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): RobertaIntermediate(\n","          (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          (intermediate_act_fn): GELUActivation()\n","        )\n","        (output): RobertaOutput(\n","          (dense): Linear(in_features=3072, out_features=768, bias=True)\n","          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","    )\n","  )\n","  (pooler): RobertaPooler(\n","    (dense): Linear(in_features=768, out_features=768, bias=True)\n","    (activation): Tanh()\n","  )\n",")"]},"execution_count":30,"metadata":{},"output_type":"execute_result"}],"source":["phobert"]},{"cell_type":"markdown","metadata":{"id":"ia_Ut5Qzx6ey"},"source":["# Trainer"]},{"cell_type":"code","execution_count":31,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":928,"referenced_widgets":["8f84f0e0b8de40758798fa8118df6b7c","5ee994051e4849c789b37d7bd061af5a","2451a711dcc646f1afb645894fe34c99","16aa1f8014c747248134464464ffb17f","95c559119421409e876c00d8cceb4813","0c481c83c8fe48ae88388b2539762bec","be783de1746f4a5fad39dac212f8074a","32da090f098d49dfa73d057735dbe7c8","0edd388061a541369e8e29aa28bfc428","1879c306538d45bba04bc9140d782afb","4d958834b7bf4b5f9a7a453efdb0f344","6140766c709b453ea01faa8274a3e7d1","9f5d1a2981654e3e9fe1a0b6c2c94564","05c5a659b2684289bdbf67556ed79e43","21f6daa8dc55451782e8a41ee2ee9731","9399c431f6e247c7b59b8ecdf3faf1cc","b5b3631dd2164f228bb03f1707319431","3a5c6d6b0fb64383af30407b36ad418c","ffe8284bfffc4dddb261d0b3a4b2dff5","863e86811e274f9eb9b84f2cff5890c1","0703d2dd17e94dc8b3efa92e89b861fd","1d18c8d7742447b78e1f071a39d5e201","0eb23f26a88f45fe94e82fc24ad6da42","024f3cac1bee4fb48231dce71221f3f7","690fbd7f657541a5843909980c455aa0","1e7b83ea1d7e46678275346fad1a878a","c96fc0111f784e55b53f3d12dd4002d0","9b252385580f491585a0ffa34bb7e212","cf68154b5ccb4324947267a4cb36c147","c12021c270e5439dbd3ce2861fb17e2e","a40e741ffce54aefbbea11f2d3f87897","6cf1f6ad1f34473cbbbbcb9d4e5e29d7","440b539aa86b4f1ebadec224d7999b44"]},"executionInfo":{"elapsed":30149,"status":"error","timestamp":1698906144854,"user":{"displayName":"Anh Đức Bùi","userId":"02440975349884324274"},"user_tz":-420},"id":"NU5qQW8IA1a7","outputId":"54363782-60cd-4ab4-ba94-3a6b2d13584d"},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of RobertaModel were not initialized from the model checkpoint at vinai/phobert-base-v2 and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","GPU available: True (cuda), used: True\n","TPU available: False, using: 0 TPU cores\n","IPU available: False, using: 0 IPUs\n","HPU available: False, using: 0 HPUs\n","c:\\Users\\buidu\\anaconda3\\envs\\nlp\\lib\\site-packages\\pytorch_lightning\\loops\\utilities.py:72: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n","Missing logger folder: D:\\DUCBUI\\DSC\\self\\save_model\\model\\log\n","c:\\Users\\buidu\\anaconda3\\envs\\nlp\\lib\\site-packages\\pytorch_lightning\\callbacks\\model_checkpoint.py:630: Checkpoint directory D:\\DUCBUI\\DSC\\self\\save_model\\model exists and is not empty.\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","c:\\Users\\buidu\\anaconda3\\envs\\nlp\\lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n","Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","\n","  | Name      | Type             | Params\n","-----------------------------------------------\n","0 | model     | ExplainableModel | 137 M \n","1 | loss_fn   | CrossEntropyLoss | 0     \n","2 | train_acc | BinaryAccuracy   | 0     \n","3 | valid_acc | BinaryAccuracy   | 0     \n","-----------------------------------------------\n","137 M     Trainable params\n","0         Non-trainable params\n","137 M     Total params\n","549.452   Total estimated model params size (MB)\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c589be87d2044b45bd710ee3ade498f8","version_major":2,"version_minor":0},"text/plain":["Sanity Checking: |          | 0/? [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","c:\\Users\\buidu\\anaconda3\\envs\\nlp\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n","C:\\Users\\buidu\\AppData\\Local\\Temp\\ipykernel_19748\\3543186699.py:57: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n","  output.append(torch.LongTensor(span_masks))\n","c:\\Users\\buidu\\anaconda3\\envs\\nlp\\lib\\site-packages\\pytorch_lightning\\utilities\\data.py:77: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 8. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n","c:\\Users\\buidu\\anaconda3\\envs\\nlp\\lib\\site-packages\\pytorch_lightning\\utilities\\data.py:77: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 2. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n","Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","c:\\Users\\buidu\\anaconda3\\envs\\nlp\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n","c:\\Users\\buidu\\anaconda3\\envs\\nlp\\lib\\site-packages\\pytorch_lightning\\loops\\fit_loop.py:293: The number of training batches (13) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"992ff73c1f5f4d369f40cc275e11fa1c","version_major":2,"version_minor":0},"text/plain":["Training: |          | 0/? [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"97ae2a0e4f8e4c938e567c75b54a5c22","version_major":2,"version_minor":0},"text/plain":["Validation: |          | 0/? [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"4a1d0879a4534cb181d52e19b6a376d4","version_major":2,"version_minor":0},"text/plain":["Validation: |          | 0/? [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ffb3d60c37b14e6eae03d204bb333e65","version_major":2,"version_minor":0},"text/plain":["Validation: |          | 0/? [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f46a9d4e045b4f8990923f49cb0c435a","version_major":2,"version_minor":0},"text/plain":["Validation: |          | 0/? [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"43646b1140a6455496ae5e820c02670b","version_major":2,"version_minor":0},"text/plain":["Validation: |          | 0/? [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d66d9c2faf8944e8bc46f670dec17a34","version_major":2,"version_minor":0},"text/plain":["Validation: |          | 0/? [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["c:\\Users\\buidu\\anaconda3\\envs\\nlp\\lib\\site-packages\\pytorch_lightning\\trainer\\call.py:54: Detected KeyboardInterrupt, attempting graceful shutdown...\n"]}],"source":["import argparse\n","import json\n","import os\n","from functools import partial\n","\n","import pytorch_lightning as pl\n","import torch\n","\n","from pytorch_lightning import Trainer\n","from pytorch_lightning.callbacks.model_checkpoint import ModelCheckpoint\n","from pytorch_lightning.loggers import TensorBoardLogger\n","from torch.nn import functional as F\n","from torch.nn.modules import CrossEntropyLoss\n","from torch.utils.data.dataloader import DataLoader\n","from transformers import AdamW, get_linear_schedule_with_warmup, RobertaTokenizer\n","from torchmetrics import Accuracy\n","\n","import os\n","import sys\n","\n","import random\n","random.seed(0)\n","\n","class ExplainNLP(pl.LightningModule):\n","\n","    def __init__(\n","        self,\n","        args: argparse.Namespace\n","    ):\n","        \"\"\"Initialize a model, tokenizer and config.\"\"\"\n","        super().__init__()\n","        self.args = args\n","        if isinstance(args, argparse.Namespace):\n","            self.save_hyperparameters(args)\n","        self.model = ExplainableModel(args.bert_path)\n","        # self.tokenizer = RobertaTokenizer.from_pretrained(args.bert_path)\n","        self.tokenizer = AutoTokenizer.from_pretrained(args.bert_path)\n","        self.loss_fn = CrossEntropyLoss(weight=torch.tensor([args.scale_pos_neg,1.]).to(device))\n","        self.train_acc = Accuracy(task=\"binary\")\n","        self.valid_acc = Accuracy(task=\"binary\")\n","        self.acc = 0\n","        self.loss = 0\n","        self.output = []\n","        self.check_data = []\n","\n","    def configure_optimizers(self):\n","        \"\"\"Prepare optimizer and schedule (linear warmup and decay)\"\"\"\n","        model = self.model\n","        no_decay = [\"bias\", \"LayerNorm.weight\"]\n","        optimizer_grouped_parameters = [\n","            {\n","                \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n","                \"weight_decay\": self.args.weight_decay,\n","            },\n","            {\n","                \"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n","                \"weight_decay\": 0.0,\n","            },\n","        ]\n","        optimizer = AdamW(optimizer_grouped_parameters,\n","                          betas=(0.9, 0.98),  # according to RoBERTa paper\n","                          lr=self.args.lr,\n","                          eps=self.args.adam_epsilon)\n","        t_total = len(self.train_dataloader()) // self.args.accumulate_grad_batches * self.args.max_epochs\n","        scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=self.args.warmup_steps,\n","                                                    num_training_steps=t_total)\n","\n","        return [optimizer], [{\"scheduler\": scheduler, \"interval\": \"step\"}]\n","\n","    def forward(self, input_ids, start_indexs, end_indexs, span_masks):\n","        return self.model(input_ids, start_indexs, end_indexs, span_masks)\n","\n","    def compute_loss_and_acc(self, batch, mode='train'):\n","        input_ids, labels, length, start_indexs, end_indexs, span_masks = batch\n","        y = labels.view(-1)\n","        y_hat, a_ij = self.forward(input_ids, start_indexs, end_indexs, span_masks)\n","        # compute loss\n","        ce_loss = self.loss_fn(y_hat, y)\n","        reg_loss = self.args.lamb * a_ij.pow(2).sum(dim=1).mean()\n","        loss = ce_loss - reg_loss\n","        # compute acc\n","        predict_scores = F.softmax(y_hat, dim=1)\n","        predict_labels = torch.argmax(predict_scores, dim=-1)\n","        if mode == 'train':\n","            acc = self.train_acc(predict_labels, y)\n","        else:\n","            acc = self.valid_acc(predict_labels, y)\n","        # if test, save extract spans\n","        if mode == 'test':\n","            values, indices = torch.topk(a_ij, self.args.span_topk)\n","            values = values.tolist()\n","            indices = indices.tolist()\n","            for i in range(len(values)):\n","                input_ids_list = input_ids[i].tolist()\n","                origin_sentence = self.tokenizer.decode(input_ids_list, skip_special_tokens=True)\n","                self.output.append(\n","                    str(labels[i].item()) + '<->' + str(predict_labels[i].item()) + '<->' + origin_sentence + '\\n')\n","                # print()\n","                for j, span_idx in enumerate(indices[i]):\n","                    score = values[i][j]\n","                    start_index = start_indexs[span_idx]\n","                    end_index = end_indexs[span_idx]\n","                    pre = self.tokenizer.decode(input_ids_list[:start_index], skip_special_tokens=True)\n","                    high_light = self.tokenizer.decode(input_ids_list[start_index:end_index + 1],\n","                                                       skip_special_tokens=True)\n","                    post = self.tokenizer.decode(input_ids_list[end_index + 1:], skip_special_tokens=True)\n","                    span_sentence = pre + '【' + high_light + '】' + post\n","                    self.output.append(format('%.4f' % score) + \"->\" + span_sentence + '\\n')\n","                    # print(format('%.4f' % score), \"->\", span_sentence)\n","                    if j == 0:\n","                        # generate data for check progress\n","                        self.check_data.append(str(labels[i].item()) + '\\t' + high_light + '\\n')\n","                self.output.append('\\n')\n","            # print('='*30)\n","\n","        return loss, acc\n","\n","    def on_validation_epoch_end(self):\n","        # log epoch metric\n","        self.valid_acc.compute()\n","        self.log('valid_acc_end', self.valid_acc.compute())\n","\n","    def train_dataloader(self) -> DataLoader:\n","        return self.get_dataloader(\"train\")\n","\n","    def training_step(self, batch, batch_idx):\n","        loss, acc = self.compute_loss_and_acc(batch)\n","        self.loss = loss\n","        self.acc = acc\n","        self.log('lr', self.trainer.optimizers[0].param_groups[0]['lr'])\n","        self.log('train_acc', acc, on_step=True, on_epoch=False)\n","        self.log('train_loss', loss)\n","        return loss\n","\n","    def val_dataloader(self):\n","        return self.get_dataloader(\"dev\")\n","\n","    def validation_step(self, batch, batch_idx):\n","        loss, acc = self.compute_loss_and_acc(batch, mode='dev')\n","        self.loss = loss\n","        self.acc = acc\n","        self.log('valid_acc', acc, on_step=False, on_epoch=True)\n","        self.log('valid_loss', loss)\n","        return loss\n","\n","    def get_dataloader(self, prefix=\"train\") -> DataLoader:\n","        \"\"\"get training dataloader\"\"\"\n","        if prefix == \"train\":\n","          dataset = DSC_Dataset(df_train,\n","                                bert_path=args.bert_path,\n","                                max_length=self.args.max_length)\n","        else:\n","          dataset = DSC_Dataset(df_eval,\n","                                bert_path=args.bert_path,\n","                                max_length=self.args.max_length)\n","        dataloader = DataLoader(\n","            dataset=dataset,\n","            batch_size=self.args.batch_size,\n","            num_workers=self.args.workers,\n","            collate_fn=partial(collate_to_max_length, fill_values=[1, 0, 0]),\n","            drop_last=False\n","        )\n","        return dataloader\n","\n","    def test_dataloader(self):\n","        return self.get_dataloader(\"test\")\n","\n","    def test_step(self, batch, batch_idx):\n","        loss, acc = self.compute_loss_and_acc(batch, mode='test')\n","        self.loss = loss\n","        self.acc = acc\n","        return {'test_loss': loss, \"test_acc\": acc}\n","\n","    def test_epoch_end(self, outputs):\n","        with open(os.path.join(self.args.save_path, 'output.txt'), 'w', encoding='utf8') as f:\n","            f.writelines(self.output)\n","        with open(os.path.join(self.args.save_path, 'test.txt'), 'w', encoding='utf8') as f:\n","            f.writelines(self.check_data)\n","        avg_loss = torch.stack([x['test_loss'] for x in outputs]).mean()\n","        avg_acc = torch.stack([x['test_acc'] for x in outputs]).mean()\n","        tensorboard_logs = {'test_loss': avg_loss, 'test_acc': avg_acc}\n","        print(avg_loss, avg_acc)\n","        return {'val_loss': avg_loss, 'log': tensorboard_logs}\n","\n","\n","\n","\n","\n","def train(args):\n","    # if save path does not exits, create it\n","    if not os.path.exists(args.save_path):\n","        os.mkdir(args.save_path)\n","\n","    model = ExplainNLP(args)\n","\n","    # # freeze layers before layer.11.output.\n","    # layer11_found = False\n","    # for name, param in model.named_parameters():\n","    #     if name.startswith(\"intermediate.encoder.layer.11.output\"):\n","    #         layer11_found = True\n","    #     if not layer11_found:\n","    #         param.requires_grad_(False)\n","\n","    a = 0\n","    def get_a(a):\n","        a += 1\n","        return a\n","    # checkpoint_callback = ModelCheckpoint(\n","    #     dirpath=os.path.join(args.save_path, f'{get_a(a)}'),\n","    #     save_top_k=args.save_topk,\n","    #     save_last=True,\n","    #     monitor=\"valid_acc_end\",\n","    #     mode=\"max\",\n","    # )\n","    checkpoint_callback = ModelCheckpoint(\n","        dirpath=args.save_path,\n","        save_top_k=args.save_topk,\n","        save_last=True,\n","        monitor=\"valid_acc_end\",\n","        mode=\"max\",\n","        filename='{epoch}-{model.acc:.2f}-{model.loss:.2f}',\n","        save_weights_only=True,\n","    )\n","\n","    logger = TensorBoardLogger(\n","        save_dir=args.save_path,\n","        name='log'\n","    )\n","\n","    # save args\n","    with open(os.path.join(args.save_path, \"args.json\"), 'w') as f:\n","        args_dict = args.__dict__\n","        # del args_dict['tpu_cores']\n","        json.dump(args_dict, f, indent=4)\n","\n","    # trainer = Trainer(logger=logger)\n","    trainer = Trainer(callbacks=checkpoint_callback, logger=logger)\n","\n","    trainer.fit(model)\n","\n","\n","def evaluate(args):\n","    model = ExplainNLP(args)\n","    checkpoint = torch.load(args.checkpoint_path, map_location=torch.device('cuda'))\n","    model.load_state_dict(checkpoint['state_dict'])\n","    trainer = Trainer()\n","    trainer.test(model)\n","\n","\n","def main():\n","    # parser = get_parser()\n","    # # parser = Trainer.add_argparse_args(parser)\n","    # args = parser.parse_args()\n","    if args.mode == 'train':\n","        train(args)\n","    elif args.mode == 'eval':\n","        evaluate(args)\n","    else:\n","        raise Exception(\"unexpected mode!!!\")\n","\n","\n","if __name__ == '__main__':\n","    from multiprocessing import freeze_support\n","\n","    freeze_support()\n","    main()"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.18"},"widgets":{"application/vnd.jupyter.widget-state+json":{"01122d2fca3b422c893fe45b2f9defd0":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"024f3cac1bee4fb48231dce71221f3f7":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9b252385580f491585a0ffa34bb7e212","placeholder":"​","style":"IPY_MODEL_cf68154b5ccb4324947267a4cb36c147","value":"Validation DataLoader 0: 100%"}},"03498195cabc4afdb6e9216edcb5887a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"05c5a659b2684289bdbf67556ed79e43":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_ffe8284bfffc4dddb261d0b3a4b2dff5","max":10,"min":0,"orientation":"horizontal","style":"IPY_MODEL_863e86811e274f9eb9b84f2cff5890c1","value":10}},"0703d2dd17e94dc8b3efa92e89b861fd":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0ad409e11bc2492b85fac1474cdfda0d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_e1635762ae2a490ca260195156d58d89","max":678,"min":0,"orientation":"horizontal","style":"IPY_MODEL_392e24892f584c578537d451c68f2925","value":678}},"0c481c83c8fe48ae88388b2539762bec":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0e7d3d33788c4f75a29d6b59967ccbe8":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0eb23f26a88f45fe94e82fc24ad6da42":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_024f3cac1bee4fb48231dce71221f3f7","IPY_MODEL_690fbd7f657541a5843909980c455aa0","IPY_MODEL_1e7b83ea1d7e46678275346fad1a878a"],"layout":"IPY_MODEL_c96fc0111f784e55b53f3d12dd4002d0"}},"0edd388061a541369e8e29aa28bfc428":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"13cba4ab9e184cebbbc2e1dd3dc2259f":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"16aa1f8014c747248134464464ffb17f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1879c306538d45bba04bc9140d782afb","placeholder":"​","style":"IPY_MODEL_4d958834b7bf4b5f9a7a453efdb0f344","value":" 1/1 [00:00&lt;00:00,  1.16it/s]"}},"1879c306538d45bba04bc9140d782afb":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"19d42d40dc414d96a60ef7fbfae234e8":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_6496d328ec194aa696eab15cecd8d2a1","IPY_MODEL_0ad409e11bc2492b85fac1474cdfda0d","IPY_MODEL_434fcd35068c4475a0854ed38740f44a"],"layout":"IPY_MODEL_13cba4ab9e184cebbbc2e1dd3dc2259f"}},"1d18c8d7742447b78e1f071a39d5e201":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"1e7b83ea1d7e46678275346fad1a878a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6cf1f6ad1f34473cbbbbcb9d4e5e29d7","placeholder":"​","style":"IPY_MODEL_440b539aa86b4f1ebadec224d7999b44","value":" 1/1 [00:00&lt;00:00,  8.71it/s]"}},"20fd13d9346d46d2bc536d6beb303b2c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_ff88b2db12c94152979110eed88e6a88","max":895321,"min":0,"orientation":"horizontal","style":"IPY_MODEL_de65fc6e1dad482fb047d8d13a889167","value":895321}},"21f6daa8dc55451782e8a41ee2ee9731":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0703d2dd17e94dc8b3efa92e89b861fd","placeholder":"​","style":"IPY_MODEL_1d18c8d7742447b78e1f071a39d5e201","value":" 10/10 [00:03&lt;00:00,  3.27it/s, v_num=0]"}},"2451a711dcc646f1afb645894fe34c99":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_32da090f098d49dfa73d057735dbe7c8","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_0edd388061a541369e8e29aa28bfc428","value":1}},"25746fcd276c44b8a637aa7cc3b2ba27":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"26543a0fa8154ab89aeb609432daf7c3":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"27a5257ad0dd4f2fbded6a7a0092542f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7ace8cab1a8f405d93fb50a2a902edb0","placeholder":"​","style":"IPY_MODEL_66cfc1ebd3be44dabc2a45fd6818545b","value":"Downloading (…)solve/main/bpe.codes: 100%"}},"27c097839a8f4befab40d55830ff5bda":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_8e771d9ec13a4a91b98fbf1a2e9c4c15","IPY_MODEL_72c6a9d448ed4ab684ce9349f2844e71","IPY_MODEL_bec994eba6d7404f9e3563a2a00f6c1b"],"layout":"IPY_MODEL_8ce7f75193a744d9aafa624322c5bd26"}},"2b7730212a7f4c26a79ed591dc805b04":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_704685f13e0d44e7ba1080e02599f2a7","placeholder":"​","style":"IPY_MODEL_c126d05c297841439f0b852867388ffb","value":"Downloading pytorch_model.bin: 100%"}},"32da090f098d49dfa73d057735dbe7c8":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":"2","flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"392e24892f584c578537d451c68f2925":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"3a5c6d6b0fb64383af30407b36ad418c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"41cd42366ae94ec3be4491277641d2b6":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"434fcd35068c4475a0854ed38740f44a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_965e38a7014c42e483e60b5d3c0771f0","placeholder":"​","style":"IPY_MODEL_53328ff985554e288124d0a5c9c9d24f","value":" 678/678 [00:00&lt;00:00, 44.9kB/s]"}},"440b539aa86b4f1ebadec224d7999b44":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"450ca25852934038815c4507678e7960":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_95b01ecce14a4338b9714ebeed252386","max":1135173,"min":0,"orientation":"horizontal","style":"IPY_MODEL_c1a1f545fb4c456c8fcc23b88c70fc16","value":1135173}},"4ac8183d982943b983635358d416e67a":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4d958834b7bf4b5f9a7a453efdb0f344":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"53328ff985554e288124d0a5c9c9d24f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"54fe938affc94b1091e300fa005b416f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_ccebef5e53f4445fafcca8ac5f25f313","IPY_MODEL_20fd13d9346d46d2bc536d6beb303b2c","IPY_MODEL_f556b3ee62ab41db8e23d225423d15b4"],"layout":"IPY_MODEL_c5aee511ed77462badc2084f2bdc3173"}},"55d6853bc854436ea85bb2fe6d2ac444":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_27a5257ad0dd4f2fbded6a7a0092542f","IPY_MODEL_450ca25852934038815c4507678e7960","IPY_MODEL_6b268b0211d543ddbd33e81ded423d1a"],"layout":"IPY_MODEL_9013ec23f4be4310a3257e7821ac21c2"}},"5ed20875c5d64a91822b6ef1118dca9d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_2b7730212a7f4c26a79ed591dc805b04","IPY_MODEL_c6fbe38396aa4b2aa301959deea7521f","IPY_MODEL_d8a475e1c2814823add0a0b9a3e71bcc"],"layout":"IPY_MODEL_25746fcd276c44b8a637aa7cc3b2ba27"}},"5ee994051e4849c789b37d7bd061af5a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0c481c83c8fe48ae88388b2539762bec","placeholder":"​","style":"IPY_MODEL_be783de1746f4a5fad39dac212f8074a","value":"Sanity Checking DataLoader 0: 100%"}},"6140766c709b453ea01faa8274a3e7d1":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_9f5d1a2981654e3e9fe1a0b6c2c94564","IPY_MODEL_05c5a659b2684289bdbf67556ed79e43","IPY_MODEL_21f6daa8dc55451782e8a41ee2ee9731"],"layout":"IPY_MODEL_9399c431f6e247c7b59b8ecdf3faf1cc"}},"6496d328ec194aa696eab15cecd8d2a1":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6bfd3395e8f342cdae384ddd697ffabb","placeholder":"​","style":"IPY_MODEL_ff8a5e1fd6ca4f70a6d3ac7656a7c419","value":"Downloading (…)lve/main/config.json: 100%"}},"66cfc1ebd3be44dabc2a45fd6818545b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"690fbd7f657541a5843909980c455aa0":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_c12021c270e5439dbd3ce2861fb17e2e","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_a40e741ffce54aefbbea11f2d3f87897","value":1}},"6b268b0211d543ddbd33e81ded423d1a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_26543a0fa8154ab89aeb609432daf7c3","placeholder":"​","style":"IPY_MODEL_942d24158cb541edb8342b66156e246d","value":" 1.14M/1.14M [00:00&lt;00:00, 9.88MB/s]"}},"6bfd3395e8f342cdae384ddd697ffabb":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6cf1f6ad1f34473cbbbbcb9d4e5e29d7":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"704685f13e0d44e7ba1080e02599f2a7":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"72c6a9d448ed4ab684ce9349f2844e71":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_01122d2fca3b422c893fe45b2f9defd0","max":3132320,"min":0,"orientation":"horizontal","style":"IPY_MODEL_c2eb568196684ed992f8666e8374354f","value":3132320}},"7ace8cab1a8f405d93fb50a2a902edb0":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"863e86811e274f9eb9b84f2cff5890c1":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"87b41e0663444295a7bb24a9e7528832":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8ce7f75193a744d9aafa624322c5bd26":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8e771d9ec13a4a91b98fbf1a2e9c4c15":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_87b41e0663444295a7bb24a9e7528832","placeholder":"​","style":"IPY_MODEL_d71423216571427da86d93d4b7f07088","value":"Downloading (…)/main/tokenizer.json: 100%"}},"8f84f0e0b8de40758798fa8118df6b7c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_5ee994051e4849c789b37d7bd061af5a","IPY_MODEL_2451a711dcc646f1afb645894fe34c99","IPY_MODEL_16aa1f8014c747248134464464ffb17f"],"layout":"IPY_MODEL_95c559119421409e876c00d8cceb4813"}},"9013ec23f4be4310a3257e7821ac21c2":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"933f197010db41b2a4528f630a3a0c11":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"935018870c144874ad93a7e0389b0d73":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9399c431f6e247c7b59b8ecdf3faf1cc":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":"inline-flex","flex":null,"flex_flow":"row wrap","grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"100%"}},"942d24158cb541edb8342b66156e246d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"95b01ecce14a4338b9714ebeed252386":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"95c559119421409e876c00d8cceb4813":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":"inline-flex","flex":null,"flex_flow":"row wrap","grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":"hidden","width":"100%"}},"965e38a7014c42e483e60b5d3c0771f0":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9b252385580f491585a0ffa34bb7e212":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9e96ed983ce045f1ac00fedbbb9a47c1":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9f5d1a2981654e3e9fe1a0b6c2c94564":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b5b3631dd2164f228bb03f1707319431","placeholder":"​","style":"IPY_MODEL_3a5c6d6b0fb64383af30407b36ad418c","value":"Epoch 0: 100%"}},"a40e741ffce54aefbbea11f2d3f87897":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"a525f1ffe7cc428abefb2fc494419ef4":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b1e33d0b5c7542dd9e028432bf3cd0bf":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b5b3631dd2164f228bb03f1707319431":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"be783de1746f4a5fad39dac212f8074a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"bec994eba6d7404f9e3563a2a00f6c1b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0e7d3d33788c4f75a29d6b59967ccbe8","placeholder":"​","style":"IPY_MODEL_a525f1ffe7cc428abefb2fc494419ef4","value":" 3.13M/3.13M [00:00&lt;00:00, 28.5MB/s]"}},"c12021c270e5439dbd3ce2861fb17e2e":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":"2","flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c126d05c297841439f0b852867388ffb":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c1a1f545fb4c456c8fcc23b88c70fc16":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"c2eb568196684ed992f8666e8374354f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"c5aee511ed77462badc2084f2bdc3173":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c6fbe38396aa4b2aa301959deea7521f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_935018870c144874ad93a7e0389b0d73","max":540322347,"min":0,"orientation":"horizontal","style":"IPY_MODEL_03498195cabc4afdb6e9216edcb5887a","value":540322347}},"c96fc0111f784e55b53f3d12dd4002d0":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":"inline-flex","flex":null,"flex_flow":"row wrap","grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":"hidden","width":"100%"}},"ccebef5e53f4445fafcca8ac5f25f313":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9e96ed983ce045f1ac00fedbbb9a47c1","placeholder":"​","style":"IPY_MODEL_b1e33d0b5c7542dd9e028432bf3cd0bf","value":"Downloading (…)solve/main/vocab.txt: 100%"}},"cf68154b5ccb4324947267a4cb36c147":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d71423216571427da86d93d4b7f07088":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d8a475e1c2814823add0a0b9a3e71bcc":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_41cd42366ae94ec3be4491277641d2b6","placeholder":"​","style":"IPY_MODEL_fe2162d8a06a49939125ad03e8dee2eb","value":" 540M/540M [00:03&lt;00:00, 131MB/s]"}},"de65fc6e1dad482fb047d8d13a889167":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"e1635762ae2a490ca260195156d58d89":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f556b3ee62ab41db8e23d225423d15b4":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4ac8183d982943b983635358d416e67a","placeholder":"​","style":"IPY_MODEL_933f197010db41b2a4528f630a3a0c11","value":" 895k/895k [00:00&lt;00:00, 4.79MB/s]"}},"fe2162d8a06a49939125ad03e8dee2eb":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ff88b2db12c94152979110eed88e6a88":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ff8a5e1fd6ca4f70a6d3ac7656a7c419":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ffe8284bfffc4dddb261d0b3a4b2dff5":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":"2","flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}}}}},"nbformat":4,"nbformat_minor":0}
